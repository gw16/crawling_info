{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reserved-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import db\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "context=ssl._create_unverified_context()\n",
    "\n",
    "import schedule\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "charged-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrebase\n",
    "import datetime\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "closing-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser():\n",
    "\n",
    "    url ='https://thinkyou.co.kr/contest/sector.asp'\n",
    "\n",
    "    browser = Chrome('./chromedriver')\n",
    "\n",
    "    delay=3\n",
    "    browser.implicitly_wait(delay)\n",
    "\n",
    "    browser.get(url)\n",
    "\n",
    "    browser.maximize_window()\n",
    "\n",
    "    body = browser.find_element_by_tag_name('body')\n",
    "\n",
    "    try :\n",
    "\n",
    "        browser.find_elements_by_xpath('//*[@id=\"searchFrm\"]/div/dl[1]/dd/p[6]/label/span')[0].click()\n",
    "        browser.find_elements_by_xpath('//*[@id=\"searchFrm\"]/div/dl[2]/dd/p[1]/label/span')[0].click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "    while True:\n",
    "        last_height = browser.execute_script('return document.documentElement.scrollHeight')\n",
    "\n",
    "        for i in range(3):\n",
    "            body.send_keys(Keys.END)\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = browser.execute_script('return document.documentElement.scrollHeight')\n",
    "        if new_height == last_height:\n",
    "            break;\n",
    "\n",
    "    page = browser.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "familiar-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling():\n",
    "    soup = browser()\n",
    "\n",
    "    len_day = 20\n",
    "\n",
    "    links_bef = []\n",
    "    titles_bef = []\n",
    "    dday_bef = []\n",
    "    inst_bef = []\n",
    "    start_bef = []\n",
    "    end_bef = []\n",
    "\n",
    "    links_aft = []\n",
    "    titles_aft = []\n",
    "    inst_aft = []\n",
    "\n",
    "    for i in range(len_day):\n",
    "        t = soup.select(' .title > a > dl > dt ')[i].text\n",
    "        fin = soup.select(' .statNew > p ')[i].text\n",
    "\n",
    "\n",
    "        if fin == '마감':\n",
    "            base_url = 'https://thinkyou.co.kr'\n",
    "            titles_aft.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "            inst_aft.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "\n",
    "            links_aft.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "        else:\n",
    "            stand = soup.select(' .statNew')[i].text.split('D')[1]\n",
    "\n",
    "\n",
    "            if stand == '-day':\n",
    "                num = 0\n",
    "                base_url = 'https://thinkyou.co.kr'\n",
    "                titles_bef.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "                inst_bef.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "                dday_bef.append(num)\n",
    "                links_bef.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "                a = i * 2\n",
    "                start_bef.append(soup.select(' .etc')[a].text[:8])\n",
    "                end_bef.append(soup.select(' .etc')[a].text[11:])\n",
    "\n",
    "            else:\n",
    "\n",
    "                base_url = 'https://thinkyou.co.kr'\n",
    "                titles_bef.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "                inst_bef.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "                dday_bef.append(soup.select(' .statNew')[i].text.split('-')[1])\n",
    "                links_bef.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "                a = i * 2\n",
    "                start_bef.append(soup.select(' .etc')[a].text[:8])\n",
    "                end_bef.append(soup.select(' .etc')[a].text[11:])\n",
    "                \n",
    "    print(inst_bef)\n",
    "\n",
    "    tabl_data_bef = {'title': titles_bef, 'notice': start_bef, 'deadline': end_bef, \n",
    "                     'dday': dday_bef, 'sponsor': inst_bef, 'title2': titles_bef, 'link': links_bef}\n",
    "    print(tabl_data_bef)\n",
    "\n",
    "    df_bef = pd.DataFrame(tabl_data_bef, columns=['type', 'qualification', 'title', \n",
    "                                                  'notice', 'deadline', 'dday', 'sponsor', 'title2', 'link'])\n",
    "    \n",
    "    df_bef['type'] = '2 공모전'\n",
    "    \n",
    "    \n",
    "    df_bef['qualification'] = '대학(원)생'\n",
    "\n",
    "\n",
    "    return df_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "alternative-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser2():\n",
    "    url_base = 'https://www.thinkcontest.com/Contest/CateField.html?page=1&c=11'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url_base, headers=headers)\n",
    "    soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "    key = ['과학/공학', '게임/소프트웨어']\n",
    "    links = []\n",
    "    titles = []\n",
    "    dday = []\n",
    "    inst = []\n",
    "    dates = []\n",
    "    k = 1\n",
    "    \n",
    "    while k <= 10:\n",
    "        url = 'https://www.thinkcontest.com/Contest/CateField.html?page=' + str(k) + '&c=11'\n",
    "        base_url = 'https://www.thinkcontest.com/'\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        len_link = len(soup.select(' .txt-left > .contest-title > a'))\n",
    "        for i in range(len_link):\n",
    "            if soup.select(' td > span ')[i].text.replace('\\n', '') == '마감':\n",
    "                break\n",
    "            else:\n",
    "                titles.append(soup.select(' .txt-left > .contest-title > a')[i].text)\n",
    "                links.append(base_url + soup.select('.txt-left > .contest-title > a ')[i]['href'])\n",
    "                dday.append(soup.select(' td > p ')[i].text.split('-')[1])\n",
    "        k=k+1\n",
    "                            \n",
    "    str_date = []\n",
    "    end_date = []\n",
    "    participate = []\n",
    "    for i in range(len(links)):\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(links[i], headers=headers)\n",
    "        soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        html = soup.select(' tr')\n",
    "        text = str(html).replace('\\n', '')\n",
    "        certi = re.compile('참가자격' + '.{200}')\n",
    "        test = certi.findall(text)[0]\n",
    "        partis = []\n",
    "        if '제한없음' in test:\n",
    "            partis.append('대학(원)생')\n",
    "            pass\n",
    "        elif '일반인' in test:\n",
    "            partis.append('대학(원)생')\n",
    "            pass\n",
    "        elif '국내외 석학과 연구진' in test:\n",
    "            partis.append('대학원생')\n",
    "            pass\n",
    "        elif '대학생' in test:\n",
    "            if '대학원생' in test:\n",
    "                partis.append('대학(원)생')\n",
    "                pass\n",
    "            else :\n",
    "                partis.append('대학생')\n",
    "                pass\n",
    "        elif '대학원생' in test:\n",
    "            partis.append('대학원생')\n",
    "        else : \n",
    "            pass\n",
    "            \n",
    "\n",
    "        participant = str(partis).replace('[', '').replace(']', '').replace(\"'\", \"\")\n",
    "        start = re.compile('접수기간' + '.{19}')\n",
    "        strdate = start.findall(text)[0].split('<td>')[1]\n",
    "        end = re.compile('접수기간' + '.{32}')\n",
    "        enddate = end.findall(text)[0].split('~')[1].replace(' ', '')\n",
    "        participate.append(participant)\n",
    "        str_date.append(strdate)\n",
    "        end_date.append(enddate)\n",
    "        inst.append(soup.select(' tbody > tr > td ')[0].text)\n",
    "        \n",
    "        \n",
    "\n",
    "    tabl_data = {'title': titles, 'notice': str_date, 'deadline': end_date, 'dday': dday,\n",
    "                 'qualification': participate, 'sponsor': inst, 'title2': titles,'link': links}\n",
    "\n",
    "    df2 = pd.DataFrame(tabl_data, columns=['type', 'qualification', 'title', \n",
    "                                           'notice', 'deadline', 'dday', 'sponsor', 'title2', 'link'])\n",
    "    df2['type'] = '2 공모전'\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "expired-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_contest():\n",
    "    you = crawling()\n",
    "    good = browser2()\n",
    "    contest = pd.concat([you, good])\n",
    "    contest_df = contest.reset_index(drop=True)\n",
    "    contest_df['title'] = contest_df['title'].str.strip()\n",
    "    contest_mid = contest_df.drop_duplicates(['title'], keep='first')\n",
    "    contest_mid2 = contest_mid.reset_index(drop = True)\n",
    "    contest_mid2['dday'] = contest_mid2['dday'].astype(int)\n",
    "    contest_fin = contest_mid2.sort_values(by ='dday')\n",
    "    final = contest_fin.reset_index(drop = True)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dying-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser_job():\n",
    "\n",
    "    url ='https://www.jobkorea.co.kr/starter/?chkSubmit=1&schCareer=&schLocal=&schPart=10016&schMajor=&schEduLevel=5&schWork=&schCType=&isSaved=1&LinkGubun=0&LinkNo=0&Page=1&schType=0&schGid=0&schOrderBy=0&schTxt='\n",
    "    browser = Chrome('./chromedriver')\n",
    "    delay=3\n",
    "    browser.implicitly_wait(delay)\n",
    "    browser.get(url) \n",
    "    browser.maximize_window()\n",
    "    page = browser.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "large-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobkorea():\n",
    "    soup = browser_job()\n",
    "    dday = []\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    insts = []\n",
    "    start_list = [] \n",
    "    end_list = []\n",
    "    element_num = len(soup.select(' .tit > .link > span'))\n",
    "    cnt = int(soup.select(' #TabIngCount')[0].text.replace('(', '').replace(')', '').replace(',', ''))\n",
    "    print(element_num)\n",
    "    print(cnt)\n",
    "    if cnt % element_num == 0:\n",
    "        page_num = cnt / element_num\n",
    "    else :\n",
    "        page_num = int(cnt / element_num) + 1\n",
    "    page_num = int(page_num)\n",
    "    print(page_num)\n",
    "    time.sleep(3)\n",
    "    for k in range(1,page_num+1):\n",
    "        print(k)\n",
    "        url= 'https://www.jobkorea.co.kr/starter/?chkSubmit=1&schCareer=&schLocal=&schPart=10016&schMajor=&schEduLevel=5&schWork=&schCType=&isSaved=1&LinkGubun=0&LinkNo=0&Page=' + str(k) +'&schType=0&schGid=0&schOrderBy=0&schTxt='\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'} \n",
    "        re = requests.get(url, headers=headers)\n",
    "        so = BeautifulSoup(re.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        length = len(so.select(' .tit > .link '))\n",
    "\n",
    "        for i in range(length):        \n",
    "            base_url = 'http://www.jobkorea.co.kr'\n",
    "            titles.append(soup.select(' .tit > .link > span')[i].text)\n",
    "            insts.append(soup.select(' .coTit > .coLink')[i].text)\n",
    "            links.append(base_url + soup.select(' .tit > a')[i+1]['href'])\n",
    "    for i in range(len(links)):\n",
    "        time.sleep(3)\n",
    "        headers_new = {'User-Agent':'Mozilla/5.0'} \n",
    "        res_new = requests.get(links[i], headers=headers_new)\n",
    "        soup_new = BeautifulSoup(res_new.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        dday.append(soup_new.select('.devRemainCount > .tahoma')[0].text)\n",
    "\n",
    "        start_list.append(str(parse(soup_new.select(' .date > dd ')[0].text[:10]))[2:10])\n",
    "\n",
    "        end_list.append(str(parse(soup_new.select(' .date > dd ')[1].text[:10]))[2:10])\n",
    "    print(\"crawling_finish\")\n",
    "\n",
    "    tabl_data_bef = {'title': titles, 'notice': start_list, 'deadline': end_list, 'dday':dday, 'sponsor':insts, 'title2':titles, 'link':links}\n",
    "    df = pd.DataFrame(tabl_data_bef, columns=['type','qualification', 'title','notice','deadline','dday','sponsor', 'title2', 'link'])\n",
    "\n",
    "    df['type'] = '5 취업'\n",
    "    df['qualification'] = '대학생'\n",
    "    \n",
    "    print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "broke-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser_job_gra():\n",
    "\n",
    "    url ='https://www.jobkorea.co.kr/starter/?chkSubmit=1&schCareer=&schLocal=&schPart=10016&schMajor=&schEduLevel=6&schWork=&schCType=&isSaved=1&LinkGubun=0&LinkNo=0&Page=1&schType=0&schGid=0&schOrderBy=0&schTxt='\n",
    "    browser = Chrome('./chromedriver')\n",
    "    delay=3\n",
    "    browser.implicitly_wait(delay)\n",
    "    browser.get(url) \n",
    "    browser.maximize_window()\n",
    "    page = browser.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "affiliated-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobkorea_gra():\n",
    "    soup = browser_job_gra()\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    insts = []\n",
    "    start_list = [] \n",
    "    end_list = []\n",
    "    dday = []\n",
    "    element_num = len(soup.select(' .tit > .link > span'))\n",
    "    cnt = int(soup.select(' #TabIngCount')[0].text.replace('(', '').replace(')', '').replace(',', ''))\n",
    "    print(element_num)\n",
    "    print(cnt)\n",
    "    if cnt % element_num == 0:\n",
    "        page_num = cnt / element_num\n",
    "    else :\n",
    "        page_num = int(cnt / element_num) + 1\n",
    "\n",
    "    page_num = int(page_num)\n",
    "    print(page_num)\n",
    "    time.sleep(3)\n",
    "    for k in range(1,page_num+1):\n",
    "        print(k)\n",
    "        url= 'https://www.jobkorea.co.kr/starter/?chkSubmit=1&schCareer=&schLocal=&schPart=10016&schMajor=&schEduLevel=6&schWork=&schCType=&isSaved=1&LinkGubun=0&LinkNo=0&Page=' + str(k) +'&schType=0&schGid=0&schOrderBy=0&schTxt='\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'} \n",
    "        re = requests.get(url, headers=headers)\n",
    "        so = BeautifulSoup(re.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        length = len(so.select(' .tit > .link '))\n",
    "\n",
    "        for i in range(length):        \n",
    "            base_url = 'http://www.jobkorea.co.kr'\n",
    "            titles.append(soup.select(' .tit > .link > span')[i].text)\n",
    "            insts.append(soup.select(' .coTit > .coLink')[i].text)\n",
    "            links.append(base_url + soup.select(' .tit > a')[i+1]['href'])\n",
    "    for i in range(len(links)):\n",
    "        time.sleep(3)\n",
    "        headers_new = {'User-Agent':'Mozilla/5.0'} \n",
    "        res_new = requests.get(links[i], headers=headers_new)\n",
    "        soup_new = BeautifulSoup(res_new.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        dday.append(soup_new.select('.devRemainCount > .tahoma')[0].text)\n",
    "\n",
    "        start_list.append(str(parse(soup_new.select(' .date > dd ')[0].text[:10]))[2:10])\n",
    "\n",
    "        end_list.append(str(parse(soup_new.select(' .date > dd ')[1].text[:10]))[2:10])\n",
    "    \n",
    "    print(\"crawling_finish\")\n",
    "\n",
    "    tabl_data_bef = {'title': titles, 'notice': start_list, 'deadline': end_list, 'dday':dday, 'sponsor':insts, 'title2':titles, 'link':links}\n",
    "    df = pd.DataFrame(tabl_data_bef, columns=['type','qualification', 'title','notice','deadline','dday','sponsor', 'title2', 'link'])\n",
    "\n",
    "    df['type'] = '5 취업'\n",
    "    df['qualification'] = '대학원생'\n",
    "\n",
    "    \n",
    "    print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "affected-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_job():\n",
    "    uni = jobkorea()\n",
    "    gra = jobkorea_gra()\n",
    "    job = pd.concat([uni, gra])\n",
    "    job_df = job.reset_index(drop=True)\n",
    "    job_df['title'] = job_df['title'].str.strip()\n",
    "    job_mid = job_df.drop_duplicates(['title'], keep='first')\n",
    "    job_mid2 = job_mid.reset_index(drop = True)\n",
    "    job_mid2['dday'] = job_mid2['dday'].astype(int)\n",
    "    job_fin = job_mid2.sort_values(by ='dday')\n",
    "    final_j = job_fin.reset_index(drop = True)\n",
    "    \n",
    "    return final_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_allcon():\n",
    "    a_title=[]\n",
    "    a_host=[]\n",
    "    a_terms=[]\n",
    "    a_start_bef=[]\n",
    "    a_end_bef=[]\n",
    "    a_qualification=[]\n",
    "    a_links=[]\n",
    "    a_real_links=[]\n",
    "    url2='https://www.all-con.co.kr/page'\n",
    "    for n in range(1,6):\n",
    "        base_url='https://www.all-con.co.kr/page/uni_activity.php?sc=0&st=2&sstt=&page={}'.format(n)\n",
    "        flag = False\n",
    "        url = base_url.format(n)\n",
    "        webpage = urlopen(url,context=context)\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        for i in range(1,16):\n",
    "            a_title.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > a > p')[0].get_text())\n",
    "            a_host.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td:nth-child(3) > p')[0].get_text())\n",
    "            a_terms.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > ul > li:nth-child(1) > p.info > span')[0].get_text())\n",
    "            a_qualification.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > ul > li:nth-child(2) > p.info')[0].get_text())\n",
    "            a_links.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > a')[0].get('href').lstrip('.'))\n",
    "    return a_title, a_host,a_terms, a_qualification,a_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "relevant-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allcon_real_link():\n",
    "    a_title, a_host,a_terms ,a_qualification,a_links = parse_allcon()\n",
    "    a_real_links=[]\n",
    "    url2='https://www.all-con.co.kr/page'\n",
    "    for a_link in a_links:\n",
    "        a=url2+a_link\n",
    "        a_real_links.append(a)\n",
    "    \n",
    "    return a_real_links      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stock-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allcon_days():\n",
    "    a_title, a_host,a_terms ,a_qualification,a_links = parse_allcon()\n",
    "    a_start_bef=[]\n",
    "    a_end_bef=[]\n",
    "    for a_term in a_terms:\n",
    "        a_start_day,a_end_day=a_term.split(\" ~ \")\n",
    "        a_start_bef.append(a_start_day.replace('.','-'))\n",
    "        a_end_bef.append(a_end_day.replace('.','-'))\n",
    "        \n",
    "    return a_start_bef, a_end_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "central-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_allcon():\n",
    "    a_title, a_host,a_terms ,a_qualification,a_links = parse_allcon()\n",
    "    a_real_links = allcon_real_link()\n",
    "    a_start_bef,a_end_bef=allcon_days()\n",
    "    allcon_table_data_bef = {'type':\"4 대외활동\", 'qualification': \"대학(원)생\",'title': a_title, 'notice': a_start_bef,  'deadline': a_end_bef,'dday': 0, 'sponsor':a_host, 'title2': a_title,'link':a_real_links}\n",
    "    df_allcon=pd.DataFrame(allcon_table_data_bef, columns=['type', 'qualification', 'title', 'notice', 'deadline','dday', 'sponsor', 'title2', 'link'])\n",
    "    return df_allcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "allied-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_incruit():\n",
    "    inc_title=[]\n",
    "    inc_host=[]\n",
    "    inc_terms=[]\n",
    "    inc_start_bef=[]\n",
    "    inc_end_bef=[]\n",
    "    inc_qualification=[]\n",
    "    inc_links=[]\n",
    "    inc_real_links=[]\n",
    "    base_url='https://gongmo.incruit.com/list/gongmolist.asp?ct=1&category=11'\n",
    "    webpage = urlopen(base_url,context=context)\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    for i in range(1,4):\n",
    "        inc_title.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.gmtitle > ul > a')[0].get_text())\n",
    "        inc_host.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.company')[0].get_text().lstrip('\\r\\n\\t\\t\\t\\t\\t\\t\\t').strip('\\r\\n\\t\\t\\t\\t\\t\\t\\t'))\n",
    "        inc_terms.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.due')[0].get_text())\n",
    "        inc_links.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.gmtitle > ul > a')[0].get('href'))\n",
    "        \n",
    "    return inc_title, inc_host, inc_terms, inc_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "differential-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incruit_days():\n",
    "    inc_title, inc_host, inc_terms, inc_links=parse_incruit()\n",
    "    inc_start_bef=[]\n",
    "    inc_end_bef=[]\n",
    "    for inc_term in inc_terms:\n",
    "        inc_start_day,inc_end_day=inc_term.split(\"~\")\n",
    "        inc_start_bef.append(inc_start_day.replace('.','-'))\n",
    "        inc_end_bef.append(inc_end_day.replace('.','-'))\n",
    "    return inc_start_bef, inc_end_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "visible-notion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_incruit():\n",
    "    inc_title, inc_host, inc_terms, inc_links=parse_incruit()\n",
    "    inc_start_bef, inc_end_bef=incruit_days()\n",
    "    incruit_table_bef = {'type':\"2 공모전\", 'qualification': '대학(원)생','title': inc_title, 'notice': inc_start_bef,  'deadline': inc_end_bef,'dday':0, 'sponsor':inc_host, 'title2': inc_title,'link':inc_links}\n",
    "    df_incruit=pd.DataFrame(incruit_table_bef, columns=['type', 'qualification', 'title', 'notice', 'deadline', 'dday','sponsor', 'title2', 'link'])\n",
    "    return df_incruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incorporate-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum():\n",
    "    df2 = sort_contest() # 2 공모전\n",
    "    df3 = final_incruit() # 2 공모전\n",
    "    df4 = final_allcon() # 4 대외활동\n",
    "    df5 = sort_job() # 5 취업\n",
    "    mid = pd.concat([df2,df3,df4, df5]) \n",
    "    mid_df = mid.reset_index(drop=True)\n",
    "    mid_df['title'] = mid_df['title'].str.strip()\n",
    "    fin_df = mid_df.drop_duplicates(['title'], keep='first')\n",
    "    fin = fin_df.reset_index(drop = True)\n",
    "    fin['dday'] = '0'\n",
    "    print(fin)\n",
    "\n",
    "    return fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "domestic-memphis",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sort_contest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-217cace8f485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-a86e11c47390>\u001b[0m in \u001b[0;36msum\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_contest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2 공모전\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_incruit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2 공모전\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_allcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 4 대외활동\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 5 취업\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sort_contest' is not defined"
     ]
    }
   ],
   "source": [
    "sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "theoretical-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tofb():\n",
    "    fin = sum()\n",
    "\n",
    "\n",
    "    postdata = fin.to_dict(orient=\"index\")\n",
    "    config = {\n",
    "        \"apiKey\": \"AIzaSyDIo8bt7OrCX6KYaxplvUauQdaehcjUo_0\",\n",
    "        \"authDomain\": \"activity-crawling.firebaseapp.com\",\n",
    "        \"databaseURL\": \"https://activity-crawling-default-rtdb.firebaseio.com\",\n",
    "        \"projectId\": \"activity-crawling\",\n",
    "        \"storageBucket\": \"activity-crawling.appspot.com\",\n",
    "        \"messagingSenderId\": \"608978503357\",\n",
    "        \"appId\": \"1:608978503357:web:374a269b8fa1a64888d9d4\"}\n",
    "    firebase = pyrebase.initialize_app(config)\n",
    "    db = firebase.database()\n",
    "    db.remove()\n",
    "    db.child().update(postdata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "central-authentication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한국소프트웨어산업협회', '산업통상자원부', '(주)엔티렉스-디바이스마트']\n",
      "{'title': ['2021 SW 챌린지 창업 공모전 (~4/2 마감) ', '제9회 엔지니어링산업설계대전 ', '2021 ICT 융합 프로젝트 공모전 '], 'notice': ['21-02-18', '21-02-01', '21-02-01'], 'deadline': ['21-04-02', '21-03-19', '21-03-31'], 'dday': ['42', '28', '40'], 'sponsor': ['한국소프트웨어산업협회', '산업통상자원부', '(주)엔티렉스-디바이스마트'], 'title2': ['2021 SW 챌린지 창업 공모전 (~4/2 마감) ', '제9회 엔지니어링산업설계대전 ', '2021 ICT 융합 프로젝트 공모전 '], 'link': ['https://thinkyou.co.kr/contest/sector_view.asp?idx=17023&page=1&pagesize=30&serstatus=&serdivision=&serfield=5&sertarget=0&serprizeMoney=&seritem=0&searchstr=', 'https://thinkyou.co.kr/contest/sector_view.asp?idx=16808&page=1&pagesize=30&serstatus=&serdivision=&serfield=5&sertarget=0&serprizeMoney=&seritem=0&searchstr=', 'https://thinkyou.co.kr/contest/sector_view.asp?idx=16631&page=1&pagesize=30&serstatus=&serdivision=&serfield=5&sertarget=0&serprizeMoney=&seritem=0&searchstr=']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-d006f4342f29>:8: DeprecationWarning: use driver.switch_to.alert instead\n",
      "  result = driver.switch_to_alert()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "181\n",
      "5\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "crawling_finish\n",
      "    type qualification                                          title  \\\n",
      "0    공모전        대학(원)생                   2021 SW 챌린지 창업 공모전 (~4/2 마감)   \n",
      "1    공모전        대학(원)생                                제9회 엔지니어링산업설계대전   \n",
      "2    공모전        대학(원)생                           2021 ICT 융합 프로젝트 공모전   \n",
      "3    장학금           대학생                              국가장학금 (2021년 1학기)   \n",
      "4    장학금           대학생                             국가근로장학 (2021년 1학기)   \n",
      "..   ...           ...                                            ...   \n",
      "102   취업        대학(원)생                                디지털 전략 담당자 (경력)   \n",
      "103   취업        대학(원)생                               디지털 데이터 담당자 (경력)   \n",
      "104   취업        대학(원)생  2021상반기 각분야 채용(경영기획/영업/웹,CS프로그래머/점포개발/점포오픈지원)   \n",
      "105   취업        대학(원)생                            정보보호 정규직 채용 [신입/경력]   \n",
      "106   취업        대학(원)생                               Support Engineer   \n",
      "\n",
      "           notice      deadline dday         sponsor  \\\n",
      "0        21-02-18      21-04-02   42     한국소프트웨어산업협회   \n",
      "1        21-02-01      21-03-19   28         산업통상자원부   \n",
      "2        21-02-01      21-03-31   40  (주)엔티렉스-디바이스마트   \n",
      "3    2021. 02. 03  2021. 03. 16   25          한국장학재단   \n",
      "4    2021. 02. 03  2021. 03. 16   25          한국장학재단   \n",
      "..            ...           ...  ...             ...   \n",
      "102      21-02-10      21-02-22    0           현대차증권   \n",
      "103      21-02-10      21-02-22    0           현대차증권   \n",
      "104      21-02-16      21-02-24    0       ㈜온누리에이치엔씨   \n",
      "105      21-02-16      21-02-28    0        한국모바일인증㈜   \n",
      "106      21-02-16      21-02-28    0        에릭슨엘지(주)   \n",
      "\n",
      "                                            title2  \\\n",
      "0                    2021 SW 챌린지 창업 공모전 (~4/2 마감)    \n",
      "1                                 제9회 엔지니어링산업설계대전    \n",
      "2                            2021 ICT 융합 프로젝트 공모전    \n",
      "3                                국가장학금 (2021년 1학기)   \n",
      "4                               국가근로장학 (2021년 1학기)   \n",
      "..                                             ...   \n",
      "102                                디지털 전략 담당자 (경력)   \n",
      "103                               디지털 데이터 담당자 (경력)   \n",
      "104  2021상반기 각분야 채용(경영기획/영업/웹,CS프로그래머/점포개발/점포오픈지원)   \n",
      "105                            정보보호 정규직 채용 [신입/경력]   \n",
      "106                               Support Engineer   \n",
      "\n",
      "                                                  link  \n",
      "0    https://thinkyou.co.kr/contest/sector_view.asp...  \n",
      "1    https://thinkyou.co.kr/contest/sector_view.asp...  \n",
      "2    https://thinkyou.co.kr/contest/sector_view.asp...  \n",
      "3    https://www.dreamspon.com/scholarship/scholars...  \n",
      "4    https://www.dreamspon.com/scholarship/scholars...  \n",
      "..                                                 ...  \n",
      "102  http://www.jobkorea.co.kr/Recruit/GI_Read/3392...  \n",
      "103  http://www.jobkorea.co.kr/Recruit/GI_Read/3392...  \n",
      "104  http://www.jobkorea.co.kr/Recruit/GI_Read/3392...  \n",
      "105  http://www.jobkorea.co.kr/Recruit/GI_Read/3391...  \n",
      "106  http://www.jobkorea.co.kr/Recruit/GI_Read/3391...  \n",
      "\n",
      "[107 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "tofb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job():\n",
    "    now = datetime.now()\n",
    "    print(now)\n",
    "    tofb()\n",
    "    print(\"end\")\n",
    "\n",
    "\n",
    "schedule.every().day.at(\"00:00\").do(job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-sampling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
