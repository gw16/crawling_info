{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "entertaining-fireplace",
   "metadata": {},
   "source": [
    "pyrebase4 (필수)\n",
    "firebase-admin\n",
    "python-firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "textile-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import db\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "located-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "import schedule\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "velvet-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "academic-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser():\n",
    "\n",
    "    url ='https://thinkyou.co.kr/contest/sector.asp'\n",
    "\n",
    "    browser = Chrome(r\"C:\\Users\\user\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "    delay=3\n",
    "    browser.implicitly_wait(delay)\n",
    "\n",
    "    browser.get(url)\n",
    "\n",
    "    browser.maximize_window()\n",
    "\n",
    "    body = browser.find_element_by_tag_name('body')\n",
    "\n",
    "    try :\n",
    "\n",
    "        browser.find_elements_by_xpath('//*[@id=\"searchFrm\"]/div/dl[1]/dd/p[6]/label/span')[0].click()\n",
    "        browser.find_elements_by_xpath('//*[@id=\"searchFrm\"]/div/dl[2]/dd/p[1]/label/span')[0].click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "    while True:\n",
    "        last_height = browser.execute_script('return document.documentElement.scrollHeight')\n",
    "\n",
    "        for i in range(3):\n",
    "            body.send_keys(Keys.END)\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = browser.execute_script('return document.documentElement.scrollHeight')\n",
    "        if new_height == last_height:\n",
    "            break;\n",
    "\n",
    "    page = browser.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "maritime-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling():\n",
    "    soup = browser()\n",
    "\n",
    "    len_day = 20\n",
    "\n",
    "    links_bef = []\n",
    "    titles_bef = []\n",
    "    dday_bef = []\n",
    "    inst_bef = []\n",
    "    start_bef = []\n",
    "    end_bef = []\n",
    "\n",
    "    links_aft = []\n",
    "    titles_aft = []\n",
    "    inst_aft = []\n",
    "\n",
    "    for i in range(len_day):\n",
    "        t = soup.select(' .title > a > dl > dt ')[i].text\n",
    "        fin = soup.select(' .statNew > p ')[i].text\n",
    "\n",
    "\n",
    "        if fin == '마감':\n",
    "            base_url = 'https://thinkyou.co.kr'\n",
    "            titles_aft.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "            inst_aft.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "\n",
    "            links_aft.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "        else:\n",
    "            stand = soup.select(' .statNew')[i].text.split('D')[1]\n",
    "\n",
    "\n",
    "            if stand == '-day':\n",
    "                base_url = 'https://thinkyou.co.kr'\n",
    "                titles_bef.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "                inst_bef.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "                dday_bef.append('D' + soup.select(' .statNew')[i].text.split('D')[1])\n",
    "                links_bef.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "                a = i * 2\n",
    "                start_bef.append(soup.select(' .etc')[a].text[:8])\n",
    "                end_bef.append(soup.select(' .etc')[a].text[11:])\n",
    "\n",
    "            else:\n",
    "\n",
    "                base_url = 'https://thinkyou.co.kr'\n",
    "                titles_bef.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "                inst_bef.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "                dday_bef.append('D' + soup.select(' .statNew')[i].text.split('D')[1])\n",
    "                links_bef.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "                a = i * 2\n",
    "                start_bef.append(soup.select(' .etc')[a].text[:8])\n",
    "                end_bef.append(soup.select(' .etc')[a].text[11:])\n",
    "\n",
    "    tabl_data_bef = {'title': titles_bef, 'notice': start_bef, 'deadline': end_bef, \n",
    "                     'dday': dday_bef, 'sponsor': inst_bef, 'title2': titles_bef, 'link': links_bef}\n",
    "\n",
    "    df_bef = pd.DataFrame(tabl_data_bef, columns=['type', 'title', 'notice', 'deadline', 'dday', 'qualification', 'sponsor', 'title2','link'])\n",
    "    \n",
    "    df_bef['type'] = '공모전'\n",
    "    df_bef['qualification'] = '대학(원)생'\n",
    "\n",
    "\n",
    "    return df_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 크롤링 데이터 합쳐서 중복 제거 하는 부분\n",
    "\n",
    "'''\n",
    "def sum():\n",
    "    df1 = crawling()\n",
    "    df2 = browser2() #이런식으로 크롤링 데이터 합쳐나가면 됨\n",
    "    mid = pd.concat([df1, df2])\n",
    "    mid_df = mid.reset_index(drop=True)\n",
    "    mid_df['title'] = mid_df['title'].str.strip()\n",
    "    fin_df = mid_df.drop_duplicates(['title'], keep='first')\n",
    "    fin = fin_df.reset_index(drop = True)\n",
    "    \n",
    "    return fin\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-panama",
   "metadata": {},
   "source": [
    "# Config 보기\n",
    "\n",
    "개인정보인지 공통된 정보인지 몰라 일단 공란으로 둠\n",
    "\n",
    "- 프로젝트 설정 > 일반 페이지 하단 '내 앱'의 Firebase SDK snippet을 CDN으로 맞춤\n",
    "\n",
    "- 아래 나온 정보에 따라 코드에 채우면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "silver-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tofb():\n",
    "    df = crawling() # sum()\n",
    "    postdata = df.to_dict()\n",
    "    config = {\n",
    "        \"apiKey\": \"\",\n",
    "        \"authDomain\": \"\",\n",
    "        \"databaseURL\": \"https://activity-crawling-default-rtdb.firebaseio.com/\",\n",
    "    \n",
    "        \"projectId\": \"\",\n",
    "        \"storageBucket\": \"\",\n",
    "        \"messagingSenderId\": \"\",\n",
    "        \"appId\": \"\"}\n",
    "    firebase = pyrebase.initialize_app(config)\n",
    "    db = firebase.database()\n",
    "    n = 0\n",
    "    l = len(df)\n",
    "    for i in df['title']:\n",
    "        db.child(n).update(df.iloc[n].to_dict())\n",
    "        n = n+1\n",
    "        print(n)\n",
    "        if n ==l : break;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "certified-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tofb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job():\n",
    "    now = datetime.now()\n",
    "    print(now)\n",
    "    tofb()\n",
    "    print(\"end\")\n",
    "\n",
    "\n",
    "schedule.every().day.at(\"00:00\").do(job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-taxation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
