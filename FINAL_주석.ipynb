{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "announced-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import db\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "import ssl\n",
    "\n",
    "import schedule\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "geographic-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrebase\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "from urllib.request import urlopen\n",
    "context=ssl._create_unverified_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "solid-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "needed-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser():\n",
    "    # 씽유 홈페이지 \n",
    "\n",
    "    url ='https://thinkyou.co.kr/contest/sector.asp'\n",
    "\n",
    "    browser = Chrome('./chromedriver')\n",
    "\n",
    "    delay=3\n",
    "    browser.implicitly_wait(delay)\n",
    "\n",
    "    browser.get(url)\n",
    "\n",
    "    browser.maximize_window()\n",
    "\n",
    "    body = browser.find_element_by_tag_name('body')\n",
    "\n",
    "    try :\n",
    "        # 과학 / 공학 * 대학(원)생 카테고리\n",
    "\n",
    "        browser.find_elements_by_xpath('//*[@id=\"searchFrm\"]/div/dl[1]/dd/p[6]/label/span')[0].click()\n",
    "        browser.find_elements_by_xpath('//*[@id=\"searchFrm\"]/div/dl[2]/dd/p[1]/label/span')[0].click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "    # 스크롤 내려가게\n",
    "    while True:\n",
    "        last_height = browser.execute_script('return document.documentElement.scrollHeight')\n",
    "\n",
    "        for i in range(3):\n",
    "            body.send_keys(Keys.END)\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = browser.execute_script('return document.documentElement.scrollHeight')\n",
    "        if new_height == last_height:\n",
    "            break;\n",
    "\n",
    "    page = browser.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "focused-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling():\n",
    "    soup = browser()\n",
    "\n",
    "    len_day = 20\n",
    "\n",
    "    links_bef = []\n",
    "    titles_bef = []\n",
    "    dday_bef = []\n",
    "    inst_bef = []\n",
    "    start_bef = []\n",
    "    end_bef = []\n",
    "\n",
    "    links_aft = []\n",
    "    titles_aft = []\n",
    "    inst_aft = []\n",
    "\n",
    "    for i in range(len_day):\n",
    "        t = soup.select(' .title > a > dl > dt ')[i].text\n",
    "        fin = soup.select(' .statNew > p ')[i].text\n",
    "        \n",
    "        \n",
    "        # 마감된 공모전 가져오고 싶은 경우\n",
    "        if fin == '마감':\n",
    "            base_url = 'https://thinkyou.co.kr'\n",
    "            titles_aft.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "            inst_aft.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "\n",
    "            links_aft.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "        else:\n",
    "            stand = soup.select(' .statNew')[i].text.split('D')[1]\n",
    "            \n",
    "            \n",
    "            # D-Day인 경우\n",
    "            if stand == '-day':\n",
    "                num = 0\n",
    "                base_url = 'https://thinkyou.co.kr'\n",
    "                titles_bef.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "                inst_bef.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "                dday_bef.append(num)\n",
    "                links_bef.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "                a = i * 2\n",
    "                st = soup.select(' .etc')[a].text[:8]\n",
    "                it = st.split('-')\n",
    "                ti = datetime.datetime(int(it[0]), int(it[1]), int(it[2]))\n",
    "                st_3 = ti.strftime('%Y-%m-%d')\n",
    "\n",
    "                start_bef.append(st_3[2:])\n",
    "                \n",
    "                ed = soup.select(' .etc')[a].text[11:]\n",
    "                it_e = ed.split('-')\n",
    "                ti_e = datetime.datetime(int(it_e[0]), int(it_e[1]), int(it_e[2]))\n",
    "                ed_3 =ti_e.strftime('%Y-%m-%d')\n",
    "                end_bef.append(ed_3[2:])\n",
    "\n",
    "            else:\n",
    "                # D-숫자 인경우\n",
    "\n",
    "                base_url = 'https://thinkyou.co.kr'\n",
    "                titles_bef.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "                inst_bef.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "                dday_bef.append(soup.select(' .statNew')[i].text.split('-')[1])\n",
    "                links_bef.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "                a = i * 2\n",
    "                st = soup.select(' .etc')[a].text[:8]\n",
    "                it = st.split('-')\n",
    "                ti = datetime.datetime(int(it[0]), int(it[1]), int(it[2]))\n",
    "                st_3 = ti.strftime('%Y-%m-%d')\n",
    "\n",
    "                start_bef.append(st_3[2:])\n",
    "                \n",
    "                ed = soup.select(' .etc')[a].text[11:]\n",
    "                it_e = ed.split('-')\n",
    "                ti_e = datetime.datetime(int(it_e[0]), int(it_e[1]), int(it_e[2]))\n",
    "                ed_3 =ti_e.strftime('%Y-%m-%d')\n",
    "                end_bef.append(ed_3[2:])\n",
    "                \n",
    "    print(inst_bef)\n",
    "\n",
    "    tabl_data_bef = {'title': titles_bef, 'notice': start_bef, 'deadline': end_bef, \n",
    "                     'dday': dday_bef, 'sponsor': inst_bef, 'title2': titles_bef, 'link': links_bef}\n",
    "    print(tabl_data_bef)\n",
    "\n",
    "    df_bef = pd.DataFrame(tabl_data_bef, columns=['type', 'qualification', 'title', \n",
    "                                                  'notice', 'deadline', 'dday', 'sponsor', 'title2', 'link'])\n",
    "    \n",
    "    df_bef['type'] = '2 공모전'\n",
    "    \n",
    "    \n",
    "    df_bef['qualification'] = '대학(원)생'\n",
    "\n",
    "\n",
    "    return df_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "offshore-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser2():\n",
    "    # 씽굿 홈페이지\n",
    "    url_base = 'https://www.thinkcontest.com/Contest/CateField.html?page=1&c=11'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url_base, headers=headers)\n",
    "    soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "    # 과학/공학 * 게임/소프트웨어 카테고리 가져오게\n",
    "    key = ['과학/공학', '게임/소프트웨어']\n",
    "    links = []\n",
    "    titles = []\n",
    "    dday = []\n",
    "    inst = []\n",
    "    dates = []\n",
    "    k = 1\n",
    "    \n",
    "    while k <= 10:\n",
    "        url = 'https://www.thinkcontest.com/Contest/CateField.html?page=' + str(k) + '&c=11'\n",
    "        base_url = 'https://www.thinkcontest.com/'\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        len_link = len(soup.select(' .txt-left > .contest-title > a'))\n",
    "        for i in range(len_link):\n",
    "            if soup.select(' td > span ')[i].text.replace('\\n', '') == '마감':\n",
    "                break\n",
    "            else:\n",
    "                titles.append(soup.select(' .txt-left > .contest-title > a')[i].text)\n",
    "                links.append(base_url + soup.select('.txt-left > .contest-title > a ')[i]['href'])\n",
    "                dday.append(soup.select(' td > p ')[i].text.split('-')[1])\n",
    "        k=k+1\n",
    "                            \n",
    "    str_date = []\n",
    "    end_date = []\n",
    "    participate = []\n",
    "    for i in range(len(links)):\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(links[i], headers=headers)\n",
    "        soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        html = soup.select(' tr')\n",
    "        text = str(html).replace('\\n', '')\n",
    "        certi = re.compile('참가자격' + '.{200}')\n",
    "        test = certi.findall(text)[0]\n",
    "        partis = []\n",
    "        # 자격 \n",
    "        if '제한없음' in test:\n",
    "            partis.append('대학(원)생')\n",
    "            pass\n",
    "        elif '일반인' in test:\n",
    "            partis.append('대학(원)생')\n",
    "            pass\n",
    "        elif '국내외 석학과 연구진' in test:\n",
    "            partis.append('대학원생')\n",
    "            pass\n",
    "        elif '대학생' in test:\n",
    "            if '대학원생' in test:\n",
    "                partis.append('대학(원)생')\n",
    "                pass\n",
    "            else :\n",
    "                partis.append('대학생')\n",
    "                pass\n",
    "        elif '대학원생' in test:\n",
    "            partis.append('대학원생')\n",
    "        else : \n",
    "            pass\n",
    "\n",
    "        participant = str(partis).replace('[', '').replace(']', '').replace(\"'\", \"\")\n",
    "        start = re.compile('접수기간' + '.{19}')\n",
    "        strdate = start.findall(text)[0].split('<td>')[1]\n",
    "        end = re.compile('접수기간' + '.{32}')\n",
    "        enddate = end.findall(text)[0].split('~')[1].replace(' ', '')\n",
    "        participate.append(participant)\n",
    "    \n",
    "        inst.append(soup.select(' tbody > tr > td ')[0].text)\n",
    "        \n",
    "        \n",
    "        it_e = strdate.split('-')\n",
    "        ti_e = datetime.datetime(int(it_e[0]), int(it_e[1]), int(it_e[2]))\n",
    "        st_3 = ti_e.strftime('%Y-%m-%d')\n",
    "        str_date.append(st_3[2:])\n",
    "                \n",
    "        it = enddate.split('-')\n",
    "        ti_e = datetime.datetime(int(it[0]), int(it[1]), int(it[2]))\n",
    "        ed_3 = ti_e.strftime('%Y-%m-%d')\n",
    "        end_date.append(ed_3[2:])\n",
    "\n",
    "        \n",
    "\n",
    "    tabl_data = {'title': titles, 'notice': str_date, 'deadline': end_date, 'dday': dday,\n",
    "                 'qualification': participate, 'sponsor': inst, 'title2': titles,'link': links}\n",
    "\n",
    "    df2 = pd.DataFrame(tabl_data, columns=['type', 'qualification', 'title', \n",
    "                                           'notice', 'deadline', 'dday', 'sponsor', 'title2', 'link'])\n",
    "    df2['type'] = '2 공모전'\n",
    "    \n",
    "    # 자격 : 대학생, 대학원생에 안 맞는 row 없애기\n",
    "    \n",
    "    for i in range(len(df2['qualification'])):\n",
    "        if df2['qualification'][i] =='':\n",
    "            df2['qualification'][i] = np.nan\n",
    "            \n",
    "    df3 = df2.dropna()\n",
    "    df4 = df3.reset_index(drop = True)\n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "built-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "올콘이라는 사이트에서 대외활동 크롤링 \n",
    "대외활동 제목= a_title, 주최사= a_host, 기간=a_terms, 링크= a_links\n",
    "기간을 크롤링 해오면 21.02.15 ~ 21.02.26 이런식으로 들어오기 때문에 ~을 기준으로 split을 사용해 시작날짜와 종료날짜 받는다.\n",
    "링크 역시 전체가 아니라 ./uni/...이런식으로 들어오기때문에 뒤에 전처리 과정에서 url2를 붙여줌으로써 전체 주소 받아옴 \n",
    "\"\"\"\n",
    "def parse_allcon():\n",
    "    a_title=[]\n",
    "    a_host=[]\n",
    "    a_terms=[]\n",
    "    a_start_bef=[]\n",
    "    a_end_bef=[]\n",
    "    a_qualification=[]\n",
    "    a_links=[]\n",
    "    a_real_links=[]\n",
    "    url2='https://www.all-con.co.kr/page'\n",
    "    for n in range(1,6):\n",
    "        base_url='https://www.all-con.co.kr/page/uni_activity.php?sc=0&st=2&sstt=&page={}'.format(n)\n",
    "        flag = False\n",
    "        url = base_url.format(n)\n",
    "        webpage = urlopen(url,context=context)\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        for i in range(1,16):\n",
    "            a_title.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > a > p')[0].get_text())\n",
    "            a_host.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td:nth-child(3) > p')[0].get_text())\n",
    "            a_terms.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > ul > li:nth-child(1) > p.info > span')[0].get_text())\n",
    "            a_qualification.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > ul > li:nth-child(2) > p.info')[0].get_text())\n",
    "            a_links.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > a')[0].get('href').lstrip('.'))\n",
    "    return a_title, a_host,a_terms, a_qualification,a_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "informative-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A태그에 href 값이 ./uni 이런식으로 들어가 있어 url2값을 앞에 붙여줌으로써 전체 링크 주소 완성 \n",
    "   링크 값을 a_links라는 리스트에 받아왔기때문에 a_link라는 단수형으로 표현하면 for문을 돌릴수 있음 \n",
    "\"\"\"\n",
    "def allcon_real_link():\n",
    "    a_title, a_host,a_terms ,a_qualification,a_links = parse_allcon()\n",
    "    a_real_links=[]\n",
    "    url2='https://www.all-con.co.kr/page'\n",
    "    for a_link in a_links:\n",
    "        a=url2+a_link\n",
    "        a_real_links.append(a)\n",
    "    \n",
    "    return a_real_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "statewide-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "전체 기간에서 ~를 기준으로 split한다. 스플릿을 하면 시작 날짜에는 21.02.14 같은 값이 들어가고 종료 날짜에는 21.02.23과 같이 들어가게 되는데 \n",
    "향후 google spreadsheet에서 전체 테이블에서 날짜를 통일하기 위해서 연도 앞에 20을 붙여 4자리가 되게 하고 날짜는 '.'과 날짜 사이에 공백을 주었음 \n",
    "\"\"\"\n",
    "def allcon_days():\n",
    "    a_title, a_host,a_terms ,a_qualification,a_links = parse_allcon()\n",
    "    a_start_bef=[]\n",
    "    a_end_bef=[]\n",
    "    for a_term in a_terms:\n",
    "        a_start_day,a_end_day=a_term.split(\" ~ \")\n",
    "        a_start_bef.append('20'+a_start_day.replace('.','. '))\n",
    "        a_end_bef.append('20'+a_end_day.replace('.','. '))\n",
    "        \n",
    "    return a_start_bef, a_end_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "economic-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_allcon():\n",
    "    a_title, a_host,a_terms ,a_qualification,a_links = parse_allcon()\n",
    "    a_real_links = allcon_real_link()\n",
    "    a_start_bef,a_end_bef=allcon_days()\n",
    "    allcon_table_data_bef = {'type':\"4 대외활동\", 'qualification': '대학(원)생','title': a_title, 'notice': a_start_bef,  'deadline': a_end_bef,'dday':0,'sponsor':a_host, 'title2': a_title,'link':a_real_links}\n",
    "    df_allcon=pd.DataFrame(allcon_table_data_bef, columns=['type', 'qualification', 'title', 'notice', 'deadline','dday','sponsor', 'title2', 'link'])\n",
    "    return df_allcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adopted-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "naming convention및 전처리 방법은  위 올콘과 동일함. \n",
    "\"\"\"\n",
    "def parse_incruit():\n",
    "    inc_title=[]\n",
    "    inc_host=[]\n",
    "    inc_terms=[]\n",
    "    inc_start_bef=[]\n",
    "    inc_end_bef=[]\n",
    "    inc_qualification=[]\n",
    "    inc_links=[]\n",
    "    inc_real_links=[]\n",
    "    base_url='https://gongmo.incruit.com/list/gongmolist.asp?ct=1&category=11'\n",
    "    webpage = urlopen(base_url,context=context)\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    for i in range(1,4):\n",
    "        inc_title.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.gmtitle > ul > a')[0].get_text())\n",
    "        inc_host.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.company')[0].get_text().lstrip('\\r\\n\\t\\t\\t\\t\\t\\t\\t').strip('\\r\\n\\t\\t\\t\\t\\t\\t\\t'))\n",
    "        inc_terms.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.due')[0].get_text())\n",
    "        inc_links.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.gmtitle > ul > a')[0].get('href'))\n",
    "        \n",
    "    return inc_title, inc_host, inc_terms, inc_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "subsequent-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incruit_days():\n",
    "    inc_title, inc_host, inc_terms, inc_links=parse_incruit()\n",
    "    inc_start_bef=[]\n",
    "    inc_end_bef=[]\n",
    "    for inc_term in inc_terms:\n",
    "        inc_start_day,inc_end_day=inc_term.split(\"~\")\n",
    "        inc_start_bef.append('20'+inc_start_day.replace('.','. '))\n",
    "        inc_end_bef.append('20'+inc_end_day.replace('.','. '))\n",
    "    return inc_start_bef, inc_end_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "suitable-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_incruit():\n",
    "    inc_title, inc_host, inc_terms, inc_links=parse_incruit()\n",
    "    inc_start_bef, inc_end_bef=incruit_days()\n",
    "    incruit_table_bef = {'type':\"2 공모전\", 'qualification': '대학(원)생','title': inc_title, 'notice': inc_start_bef,  'deadline': inc_end_bef, 'sponsor':inc_host, 'dday':0,'title2': inc_title,'link':inc_links}\n",
    "    df_incruit=pd.DataFrame(incruit_table_bef, columns=['type', 'qualification', 'title', 'notice', 'deadline','dday', 'sponsor', 'title2', 'link'])\n",
    "    return df_incruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "respective-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_contest():\n",
    "    # 공모전 카테고리 끼리 합침\n",
    "    you = crawling()\n",
    "    good = browser2()\n",
    "    incruit= final_incruit()\n",
    "    contest = pd.concat([you, good, incruit])\n",
    "    contest_df = contest.reset_index(drop=True)\n",
    "    # 중복 제거\n",
    "    contest_df['title'] = contest_df['title'].str.strip()\n",
    "    contest_mid = contest_df.drop_duplicates(['title'], keep='first')\n",
    "    contest_mid2 = contest_mid.reset_index(drop = True)\n",
    "    contest_mid2['dday'] = contest_mid2['dday'].astype(int)\n",
    "    contest_mid2['dday'] = 0\n",
    "    contest_fin = contest_mid2.sort_values(by ='dday')\n",
    "    final = contest_fin.reset_index(drop = True)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-survey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "annoying-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser_job():\n",
    "    # 잡코리아의 IT * 대학교 항목\n",
    "\n",
    "    url ='https://www.jobkorea.co.kr/starter/?chkSubmit=1&schCareer=&schLocal=&schPart=10016&schMajor=&schEduLevel=5&schWork=&schCType=&isSaved=1&LinkGubun=0&LinkNo=0&Page=1&schType=0&schGid=0&schOrderBy=0&schTxt='\n",
    "    browser = Chrome('./chromedriver')\n",
    "    delay=3\n",
    "    browser.implicitly_wait(delay)\n",
    "    browser.get(url) \n",
    "    browser.maximize_window()\n",
    "    page = browser.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cheap-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobkorea():\n",
    "    soup = browser_job()\n",
    "    dday = []\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    insts = []\n",
    "    start_list = [] \n",
    "    end_list = []\n",
    "    # 페이지 개수 구해서 넘어가게\n",
    "    element_num = len(soup.select(' .tit > .link > span'))\n",
    "    cnt = int(soup.select(' #TabIngCount')[0].text.replace('(', '').replace(')', '').replace(',', ''))\n",
    "    print(element_num)\n",
    "    print(cnt)\n",
    "    if cnt % element_num == 0:\n",
    "        page_num = cnt / element_num\n",
    "    else :\n",
    "        page_num = int(cnt / element_num) + 1\n",
    "    page_num = int(page_num)\n",
    "    print(page_num)\n",
    "    time.sleep(3)\n",
    "    for k in range(1,page_num+1):\n",
    "        print(k)\n",
    "        url= 'https://www.jobkorea.co.kr/starter/?chkSubmit=1&schCareer=&schLocal=&schPart=10016&schMajor=&schEduLevel=5&schWork=&schCType=&isSaved=1&LinkGubun=0&LinkNo=0&Page=' + str(k) +'&schType=0&schGid=0&schOrderBy=0&schTxt='\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'} \n",
    "        re = requests.get(url, headers=headers)\n",
    "        so = BeautifulSoup(re.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        length = len(so.select(' .tit > .link '))\n",
    "\n",
    "        for i in range(length):        \n",
    "            base_url = 'http://www.jobkorea.co.kr'\n",
    "            titles.append(soup.select(' .tit > .link > span')[i].text)\n",
    "            insts.append(soup.select(' .coTit > .coLink')[i].text)\n",
    "            links.append(base_url + soup.select(' .tit > a')[i+1]['href'])\n",
    "    for i in range(len(links)):\n",
    "        time.sleep(3)\n",
    "        headers_new = {'User-Agent':'Mozilla/5.0'} \n",
    "        res_new = requests.get(links[i], headers=headers_new)\n",
    "        soup_new = BeautifulSoup(res_new.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        dday.append(soup_new.select('.devRemainCount > .tahoma')[0].text)\n",
    "        \n",
    "        st = str(parse(soup_new.select(' .date > dd ')[0].text[:10]))[2:10]\n",
    "        it = st.split('-')\n",
    "        ti = datetime.datetime(int(it[0]), int(it[1]), int(it[2]))\n",
    "        st_2 = ti.strftime('%Y-%m-%d')\n",
    "        start_list.append(st_2[2:])\n",
    "        \n",
    "        ed = str(parse(soup_new.select(' .date > dd ')[1].text[:10]))[2:10]\n",
    "        it_e = ed.split('-')\n",
    "        ti_e = datetime.datetime(int(it_e[0]), int(it_e[1]), int(it_e[2]))\n",
    "        ed_2 = ti_e.strftime('%Y-%m-%d')\n",
    "        end_list.append(ed_2[2:])\n",
    "    print(\"crawling_finish\")\n",
    "\n",
    "    tabl_data_bef = {'title': titles, 'notice': start_list, 'deadline': end_list, 'dday':dday, 'sponsor':insts, 'title2':titles, 'link':links}\n",
    "    df = pd.DataFrame(tabl_data_bef, columns=['type','qualification', 'title','notice','deadline','dday','sponsor', 'title2', 'link'])\n",
    "\n",
    "    df['type'] = '5 취업'\n",
    "    df['qualification'] = '대학생'\n",
    "    \n",
    "    print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "residential-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser_job_gra():\n",
    "    # 잡코리아의 IT * 대학원생 항목\n",
    "\n",
    "    url ='https://www.jobkorea.co.kr/starter/?chkSubmit=1&schCareer=&schLocal=&schPart=10016&schMajor=&schEduLevel=6&schWork=&schCType=&isSaved=1&LinkGubun=0&LinkNo=0&Page=1&schType=0&schGid=0&schOrderBy=0&schTxt='\n",
    "    browser = Chrome('./chromedriver')\n",
    "    delay=3\n",
    "    browser.implicitly_wait(delay)\n",
    "    browser.get(url) \n",
    "    browser.maximize_window()\n",
    "    page = browser.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "documented-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobkorea_gra():\n",
    "    soup = browser_job_gra()\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    insts = []\n",
    "    start_list = [] \n",
    "    end_list = []\n",
    "    dday = []\n",
    "    element_num = len(soup.select(' .tit > .link > span'))\n",
    "    cnt = int(soup.select(' #TabIngCount')[0].text.replace('(', '').replace(')', '').replace(',', ''))\n",
    "    print(element_num)\n",
    "    print(cnt)\n",
    "    if cnt % element_num == 0:\n",
    "        page_num = cnt / element_num\n",
    "    else :\n",
    "        page_num = int(cnt / element_num) + 1\n",
    "\n",
    "    page_num = int(page_num)\n",
    "    print(page_num)\n",
    "    time.sleep(3)\n",
    "    for k in range(1,page_num+1):\n",
    "        print(k)\n",
    "        url= 'https://www.jobkorea.co.kr/starter/?chkSubmit=1&schCareer=&schLocal=&schPart=10016&schMajor=&schEduLevel=6&schWork=&schCType=&isSaved=1&LinkGubun=0&LinkNo=0&Page=' + str(k) +'&schType=0&schGid=0&schOrderBy=0&schTxt='\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'} \n",
    "        re = requests.get(url, headers=headers)\n",
    "        so = BeautifulSoup(re.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        length = len(so.select(' .tit > .link '))\n",
    "\n",
    "        for i in range(length):        \n",
    "            base_url = 'http://www.jobkorea.co.kr'\n",
    "            titles.append(soup.select(' .tit > .link > span')[i].text)\n",
    "            insts.append(soup.select(' .coTit > .coLink')[i].text)\n",
    "            links.append(base_url + soup.select(' .tit > a')[i+1]['href'])\n",
    "    for i in range(len(links)):\n",
    "        time.sleep(3)\n",
    "        headers_new = {'User-Agent':'Mozilla/5.0'} \n",
    "        res_new = requests.get(links[i], headers=headers_new)\n",
    "        soup_new = BeautifulSoup(res_new.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        dday.append(soup_new.select('.devRemainCount > .tahoma')[0].text)\n",
    "\n",
    "        st = str(parse(soup_new.select(' .date > dd ')[0].text[:10]))[2:10]\n",
    "        it = st.split('-')\n",
    "        ti = datetime.datetime(int(it[0]), int(it[1]), int(it[2]))\n",
    "        st_2 = ti.strftime('%Y-%m-%d')\n",
    "        start_list.append(st_2[2:])\n",
    "        \n",
    "        ed = str(parse(soup_new.select(' .date > dd ')[1].text[:10]))[2:10]\n",
    "        it_e = ed.split('-')\n",
    "        ti_e = datetime.datetime(int(it_e[0]), int(it_e[1]), int(it_e[2]))\n",
    "        ed_2 = ti_e.strftime('%Y-%m-%d')\n",
    "        end_list.append(ed_2[2:])\n",
    "    print(\"crawling_finish\")\n",
    "\n",
    "    tabl_data_bef = {'title': titles, 'notice': start_list, 'deadline': end_list, 'dday':dday, 'sponsor':insts, 'title2':titles, 'link':links}\n",
    "    df = pd.DataFrame(tabl_data_bef, columns=['type','qualification', 'title','notice','deadline','dday','sponsor', 'title2', 'link'])\n",
    "\n",
    "    df['type'] = '5 취업'\n",
    "    df['qualification'] = '대학원생'\n",
    "\n",
    "    \n",
    "    print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "surprising-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_dreams():\n",
    "    dday_bef = []\n",
    "    page_num = 1\n",
    "    while(page_num <=5):        \n",
    "        url = 'https://www.dreamspon.com/scholarship/scholarship02.html?page=' + str(page_num)\n",
    "        req = urllib.request.urlopen(url)\n",
    "        res = req.read()\n",
    "        soup = BeautifulSoup(res,'html.parser')\n",
    "        days = soup.select(\" .td_day > .count\")        \n",
    "        for i in range(len(days)):\n",
    "            if 'D+' in str(days[i].text):\n",
    "                pass\n",
    "            else:\n",
    "                \n",
    "                dday_bef.append((days[i].text).strip(\"D-\"))\n",
    "        page_num += 1  \n",
    "    return dday_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "healthy-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_test_dreams():\n",
    "    link_test = []\n",
    "    page_num = 1\n",
    "    while(page_num <=5):        \n",
    "        url = 'https://www.dreamspon.com/scholarship/scholarship02.html?page=' + str(page_num)\n",
    "        req = urllib.request.urlopen(url)\n",
    "        res = req.read()\n",
    "        soup = BeautifulSoup(res,'html.parser')\n",
    "        contests = soup.find_all(\"p\",class_=\"title\")\n",
    "        days = soup.select(\" .td_day > .count\")        \n",
    "        for i in range(len(days)):\n",
    "            if 'D+' in str(days[i].text):\n",
    "                pass\n",
    "            else:\n",
    "                link_test.append(str(contests[i]).strip('[<p class=\"title\"><a href=\"').strip('</a>'))\n",
    "        page_num += 1  \n",
    "    return link_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "opening-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_dreams(link_test):\n",
    "    link_bef=[]\n",
    "    page_num = 1\n",
    "    for t in range(len(link_test)):\n",
    "        link_address, title_name = link_test[t].split('\">')\n",
    "        link_ver1 = \"https://www.dreamspon.com/\" + link_address\n",
    "        link_bef.append(link_ver1)\n",
    "    page_num += 1  \n",
    "    return link_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "another-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "def titles_dreams(link_test):\n",
    "    titles_bef = [] # 행사 이름\n",
    "    page_num = 1\n",
    "    for t in range(len(link_test)):\n",
    "        link_address, title_name = link_test[t].split('\">')\n",
    "        titles_bef.append(title_name)\n",
    "    page_num += 1  \n",
    "    return titles_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "invisible-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insts_dreams():\n",
    "    inst = []\n",
    "    page_num = 1\n",
    "    while(page_num <=5):\n",
    "        url = 'https://www.dreamspon.com/scholarship/scholarship02.html?page=' + str(page_num)\n",
    "        req = urllib.request.urlopen(url)\n",
    "        res = req.read()\n",
    "        soup = BeautifulSoup(res,'html.parser')\n",
    "        idx = 1\n",
    "        while(idx<=60):\n",
    "            if 'D-' in str(soup.select(\"tr>td\")[idx+1].text):\n",
    "                inst.append(soup.select(\"tr>td\")[idx].text) #       1,5,9, 13\n",
    "            idx += 4    \n",
    "        page_num += 1\n",
    "    return inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "applicable-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_e_test_dreams(list_adress):\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver')\n",
    "    driver.implicitly_wait(10)\n",
    "    login_path ='//*[@id=\"loginForm\"]/div[1]/input'\n",
    "\n",
    "    driver.get(list_adress)\n",
    "    result = driver.switch_to_alert()\n",
    "    result.accept()\n",
    "\n",
    "    driver.find_element_by_name('mbr_id').send_keys('rainrain16@hanmail.net')\n",
    "    driver.find_element_by_name('pwd_in').send_keys('rainrain16')\n",
    "    driver.find_element_by_xpath(login_path).click()\n",
    "\n",
    "    sleep(1)\n",
    "\n",
    "    html = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "    crawl_data = str(soup.find_all(\"li\", class_= \"day\"))\n",
    "\n",
    "    return crawl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "married-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_e_days_dreams():\n",
    "    crawl=[]\n",
    "    sd_days_list = link_dreams(link_test_dreams())\n",
    "    \n",
    "    for i in range(len(sd_days_list)):\n",
    "        crawl.append(s_e_test_dreams(sd_days_list[i]))\n",
    "    return crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "individual-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_e_preprocess():\n",
    "    s_e_pre = s_e_days_dreams()\n",
    "    \n",
    "    for i in range(len(s_e_pre)):\n",
    "        s_e_pre[i] = (s_e_pre[i].strip('[<li class=\"day\" style=\"height: 70px; \">')).strip('\\n\\t')\n",
    "\n",
    "    return s_e_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "upset-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#날짜를 가지고 오는 코드인데, 상당히 까다로움. 가능한 정규표현식으로 수정할 필요 있음\n",
    "def s_e_final():\n",
    "    s_e_pre = s_e_preprocess()\n",
    "    start_bef = []\n",
    "    end_bef = []\n",
    "    for i in range(len(s_e_pre)):\n",
    "        if '(1차)' not in s_e_pre[i]:\n",
    "\n",
    "            start_day, end_day_ver1  = (s_e_pre[i]).split('~')\n",
    "            end_day_ver2, end_day_ver3 =  (end_day_ver1).split(\"<span>D\")\n",
    "            start_bef.append((start_day.strip('.')).strip('. '))\n",
    "            end_bef.append(end_day_ver2.strip('.'))\n",
    "\n",
    "        else:            \n",
    "\n",
    "            start_ver1, start_ver2 = (s_e_pre[i]).split('</span><br/>')\n",
    "            start_ver3, end_ver1 = start_ver1.split('~')\n",
    "            end_ver2, end_ver3 = end_ver1.split('<span')\n",
    "\n",
    "            start_ver4 ,end_ver4  = start_ver2.split('~')\n",
    "  \n",
    "            end_ver5, end_ver6 = end_ver4.split('<span')\n",
    "\n",
    "            \n",
    "            start_bef.append('2' + start_ver4.strip(\"(2차) \").strip(\". \").strip(\".\"))\n",
    "            end_bef.append(end_ver5.strip(\".\").strip(\" \"))\n",
    "\n",
    "    return start_bef, end_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "floppy-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_dreams():\n",
    "    dday =  days_dreams()\n",
    "    links = link_dreams(link_test_dreams())\n",
    "    title = titles_dreams(link_test_dreams())\n",
    "    start, end = s_e_final()\n",
    "    inst = insts_dreams()\n",
    "    print(len(dday), len(links), len(title), len(start), len(end), len(inst))\n",
    "    \n",
    "    tabl_data_bef = {'type':'3 장학금', 'qualification': \"대학생\",'title': title, 'notice': start,  'deadline': end, 'dday':dday, 'sponsor':inst, 'title2': title,'link':links}\n",
    "    df_bef = pd.DataFrame(tabl_data_bef, columns=['type', 'qualification', 'title', 'notice', 'deadline', 'dday', 'sponsor', 'title2', 'link'])\n",
    "    return df_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-mortality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "express-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_ck():\n",
    "    title_bef =[]\n",
    "    days_bef = []\n",
    "    start_bef = []\n",
    "    end_bef = []\n",
    "    conditions = []\n",
    "    inst_bef = []\n",
    "    link_bef = []\n",
    "    page_num = 1\n",
    "    while(page_num<=5):\n",
    "        url = 'https://www.contestkorea.com/sub/list.php?displayrow=12&Txt_sGn=1&Txt_key=all&Txt_word=&Txt_bcode=030210001&Txt_code1%5B0%5D=30&Txt_code1%5B1%5D=76&Txt_aarea=&Txt_area=&Txt_sortkey=a.int_sort&Txt_sortword=desc&Txt_chocode=&Txt_unicode=&page=' + str(page_num)\n",
    "        req = urllib.request.urlopen(url)\n",
    "        res = req.read()\n",
    "        soup = BeautifulSoup(res,'html.parser')\n",
    "        titles = soup.find_all(\"span\", class_ = \"txt\")\n",
    "        titles = list(titles)\n",
    "        condition = soup.find_all(\"span\", class_=\"condition\")\n",
    "        condition = list(condition)\n",
    "        days = soup.select(\".date > div\" )\n",
    "        days = list(days)\n",
    "        \n",
    "        inst = soup.select(\".host > .icon_1\" )\n",
    "        inst = list(inst)\n",
    "\n",
    "        for i in range(len(condition)):\n",
    "            days[i] = (str(days[i]).strip('<div>').strip(\">\").strip(\"\\t\"))\n",
    "            condition[i] = (str(condition[i]).strip('<span class=\"condition\">').strip(\"</span>\"))\n",
    "            if condition[i] != '접수종료':\n",
    "                conditions.append((condition[i]))\n",
    "                titles[i] = (str(titles[i]).strip('<span class=\"txt\">').strip(\"</'\").strip(\" \"))\n",
    "                days[i] = (str(days[i]).strip('<div>').strip('\\n\\t').strip('</li>'))\n",
    "                inst[i] = (str(inst[i]).strip('<li class=\"icon_1\"><strong>주최</strong> . ').strip('<div>'))\n",
    "                if '공모전' not in titles[i]:\n",
    "                    title_bef.append(titles[i])\n",
    "                    start_day,end_day = days[i].split('~')\n",
    "                    start_bef.append(start_day.replace(\".\", \". \"))\n",
    "                    end_bef.append(end_day.replace(\".\", \". \").strip('\\t'))\n",
    "                    inst_bef.append(inst[i])             \n",
    "        page_num+=1\n",
    "    return title_bef, start_bef, end_bef, inst_bef        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "academic-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_ck():\n",
    "    conditions=[]\n",
    "    link_bef = []\n",
    "    page_num = 1\n",
    "    while(page_num<=5):\n",
    "        url = 'https://www.contestkorea.com/sub/list.php?displayrow=12&Txt_sGn=1&Txt_key=all&Txt_word=&Txt_bcode=030210001&Txt_code1%5B0%5D=30&Txt_code1%5B1%5D=76&Txt_aarea=&Txt_area=&Txt_sortkey=a.int_sort&Txt_sortword=desc&Txt_chocode=&Txt_unicode=&page=' + str(page_num)\n",
    "        req = urllib.request.urlopen(url)\n",
    "        res = req.read()\n",
    "        soup = BeautifulSoup(res,'html.parser')\n",
    "        titles = soup.find_all(\"span\", class_ = \"txt\")\n",
    "        titles = list(titles)\n",
    "        condition = soup.find_all(\"span\", class_=\"condition\")\n",
    "        condition = list(condition)\n",
    "        for i in range(len(condition)):\n",
    "            condition[i] = (str(condition[i]).strip('<span class=\"condition\">').strip(\"</span>\"))\n",
    "            if condition[i] != '접수종료':\n",
    "                conditions.append((condition[i]))\n",
    "                titles[i] = (str(titles[i]).strip('<span class=\"txt\">').strip(\"</'\").strip(\" \"))\n",
    "                if '공모전' not in titles[i]:                 \n",
    "                    browser = Chrome('./chromedriver.exe')\n",
    "                    delay=1\n",
    "                    browser.implicitly_wait(delay)\n",
    "                    browser.get(url)\n",
    "                    browser.maximize_window()\n",
    "                    link_adress = '//*[@id=\"frm\"]/div/div[4]/ul/li['+ str(i+1) + ']/div[1]/a'\n",
    "                    browser.find_elements_by_xpath(link_adress)[0].click()\n",
    "                    sleep(0.5)\n",
    "                    link_bef.append(browser.current_url)\n",
    "                    \n",
    "                    \n",
    "        page_num+=1\n",
    "    return link_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "determined-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_ck():\n",
    "    title_bef, start_bef, end_bef, inst_bef = base_ck()\n",
    "    link_bef = link_ck()\n",
    "    \n",
    "    tabl_data_bef = {'type':\"1 경진대회\", 'qualification': \"대학(원)생\",'title':  title_bef, 'notice': start_bef,  'deadline': end_bef, 'dday':0,'sponsor':inst_bef, 'title2': title_bef,\n",
    "                     'link':link_bef}\n",
    "\n",
    "    df_bef = pd.DataFrame(tabl_data_bef, columns=['type', 'qualification', 'title', 'notice', 'deadline', 'dday', 'sponsor', 'title2', 'link'])\n",
    "    return df_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "authentic-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_job():\n",
    "    # 취업끼리 합침\n",
    "    uni = jobkorea()\n",
    "    gra = jobkorea_gra()\n",
    "    job = pd.concat([uni, gra])\n",
    "    job_df = job.reset_index(drop=True)\n",
    "    # 중복 제거\n",
    "    job_df['title'] = job_df['title'].str.strip()\n",
    "    job_mid = job_df.drop_duplicates(['title'], keep='first')\n",
    "    job_mid2 = job_mid.reset_index(drop = True)\n",
    "    job_mid2['dday'] = job_mid2['dday'].astype(int)\n",
    "    job_fin = job_mid2.sort_values(by ='dday')\n",
    "    final_j = job_fin.reset_index(drop = True)\n",
    "    \n",
    "    return final_j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-desire",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "expected-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum():\n",
    "    df1 = crawling_ck() #1 경진대회\n",
    "    df2 = sort_contest() # 2 공모전\n",
    "    df3 =  final_dreams() # 3 장학금\n",
    "    df4 = final_allcon() # 4. 대외활동\n",
    "    df5 = sort_job() # 5 취업\n",
    "    mid = pd.concat([df1, df2, df3, df4, df5]) \n",
    "    # 중복 제거\n",
    "    mid_df = mid.reset_index(drop=True)\n",
    "    mid_df['title'] = mid_df['title'].str.strip()\n",
    "    fin_df = mid_df.drop_duplicates(['title'], keep='first')\n",
    "    fin = fin_df.reset_index(drop = True)\n",
    "    fin['dday'] = '0'\n",
    "    print(fin)\n",
    "\n",
    "    return fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dependent-accordance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['과학기술정보통신부', '강원도, 원주시, 한국보건산업진흥원', '산업통상자원부', '(주)엔티렉스-디바이스마트']\n",
      "{'title': ['제2기 LMO(유전자변형생물체) SAFETY 기자단 모집  ', '2021 의료기기 창업공모전 ', '제9회 엔지니어링산업설계대전 ', '2021 ICT 융합 프로젝트 공모전 '], 'notice': ['21-02-22', '21-02-23', '21-02-01', '21-02-01'], 'deadline': ['21-03-15', '21-03-23', '21-03-19', '21-03-31'], 'dday': ['19', '27', '23', '35'], 'sponsor': ['과학기술정보통신부', '강원도, 원주시, 한국보건산업진흥원', '산업통상자원부', '(주)엔티렉스-디바이스마트'], 'title2': ['제2기 LMO(유전자변형생물체) SAFETY 기자단 모집  ', '2021 의료기기 창업공모전 ', '제9회 엔지니어링산업설계대전 ', '2021 ICT 융합 프로젝트 공모전 '], 'link': ['https://thinkyou.co.kr/contest/sector_view.asp?idx=17078&page=1&pagesize=30&serstatus=&serdivision=&serfield=5&sertarget=0&serprizeMoney=&seritem=0&searchstr=', 'https://thinkyou.co.kr/contest/sector_view.asp?idx=17067&page=1&pagesize=30&serstatus=&serdivision=&serfield=5&sertarget=0&serprizeMoney=&seritem=0&searchstr=', 'https://thinkyou.co.kr/contest/sector_view.asp?idx=16808&page=1&pagesize=30&serstatus=&serdivision=&serfield=5&sertarget=0&serprizeMoney=&seritem=0&searchstr=', 'https://thinkyou.co.kr/contest/sector_view.asp?idx=16631&page=1&pagesize=30&serstatus=&serdivision=&serfield=5&sertarget=0&serprizeMoney=&seritem=0&searchstr=']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-2c5460c2832a>:8: DeprecationWarning: use driver.switch_to.alert instead\n",
      "  result = driver.switch_to_alert()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 54 54 54 54 54\n",
      "40\n",
      "171\n",
      "5\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "crawling_finish\n",
      "     type qualification                           title    notice  deadline  \\\n",
      "0    5 취업           대학생               국내 콘텐츠 수급 경력사원 채용  21-02-17  21-03-01   \n",
      "1    5 취업           대학생  [Bio건강기능식품]마케팅팀/ IT 개발팀 경력자 모집  21-02-24  21-03-26   \n",
      "2    5 취업           대학생         (주)다인정공 각 부문 신입/경력 채용공고  21-02-24  21-03-07   \n",
      "3    5 취업           대학생                소프트웨어 QA 신입사원 채용  21-02-24  21-03-10   \n",
      "4    5 취업           대학생        [메가스터디교육] 앱 기획&운영 담당자 모집  21-02-24  21-03-26   \n",
      "..    ...           ...                             ...       ...       ...   \n",
      "166  5 취업           대학생               [신입/경력] 영어 연구원 채용  21-02-24  21-05-25   \n",
      "167  5 취업           대학생       제네시스 모바일 앱 서비스 기획 및 UX/UI  21-02-24  21-03-09   \n",
      "168  5 취업           대학생      재능그룹 IT직군 모집 (웹개발/시스템엔지니어)  21-02-23  21-03-14   \n",
      "169  5 취업           대학생      2021년 상반기 아성다이소 경력사원 수시 채용  21-02-23  21-03-02   \n",
      "170  5 취업           대학생               한세드림 전산개발팀 경력직 채용  21-02-23  21-05-24   \n",
      "\n",
      "    dday   sponsor                          title2  \\\n",
      "0      5      티캐스트               국내 콘텐츠 수급 경력사원 채용   \n",
      "1     30    ㈜쎌바이오텍  [Bio건강기능식품]마케팅팀/ IT 개발팀 경력자 모집   \n",
      "2     11     ㈜다인정공         (주)다인정공 각 부문 신입/경력 채용공고   \n",
      "3     14    ㈜누리텔레콤                소프트웨어 QA 신입사원 채용   \n",
      "4     30  메가스터디교육㈜        [메가스터디교육] 앱 기획&운영 담당자 모집   \n",
      "..   ...       ...                             ...   \n",
      "166   90   ㈜해커스어학원               [신입/경력] 영어 연구원 채용   \n",
      "167   13    현대자동차㈜       제네시스 모바일 앱 서비스 기획 및 UX/UI   \n",
      "168   18  ㈜재능이아카데미      재능그룹 IT직군 모집 (웹개발/시스템엔지니어)   \n",
      "169    6    ㈜아성다이소      2021년 상반기 아성다이소 경력사원 수시 채용   \n",
      "170   89     한세드림㈜               한세드림 전산개발팀 경력직 채용   \n",
      "\n",
      "                                                  link  \n",
      "0    http://www.jobkorea.co.kr/Recruit/GI_Read/3399...  \n",
      "1    http://www.jobkorea.co.kr/Recruit/GI_Read/3399...  \n",
      "2    http://www.jobkorea.co.kr/Recruit/GI_Read/3399...  \n",
      "3    http://www.jobkorea.co.kr/Recruit/GI_Read/3399...  \n",
      "4    http://www.jobkorea.co.kr/Recruit/GI_Read/3399...  \n",
      "..                                                 ...  \n",
      "166  http://www.jobkorea.co.kr/Recruit/GI_Read/3399...  \n",
      "167  http://www.jobkorea.co.kr/Recruit/GI_Read/3398...  \n",
      "168  http://www.jobkorea.co.kr/Recruit/GI_Read/3398...  \n",
      "169  http://www.jobkorea.co.kr/Recruit/GI_Read/3398...  \n",
      "170  http://www.jobkorea.co.kr/Recruit/GI_Read/3398...  \n",
      "\n",
      "[171 rows x 9 columns]\n",
      "6\n",
      "6\n",
      "1\n",
      "1\n",
      "crawling_finish\n",
      "   type qualification                                 title    notice  \\\n",
      "0  5 취업          대학원생               2021년 상반기 해외 석박사 R&D 채용  21-02-21   \n",
      "1  5 취업          대학원생          [하나금융융합기술원] 각 부문별 경력직 연구원 모집  21-02-20   \n",
      "2  5 취업          대학원생               [전문연구요원] 빅데이터 엔지니어 수시채용  21-02-18   \n",
      "3  5 취업          대학원생  2021년도 제1차 전문경력직(연구ㆍ전문ㆍ경력신입) 선발 모집요강  21-02-10   \n",
      "4  5 취업          대학원생            [NHN] 2021년 (신입) 전문연구요원 모집  21-01-15   \n",
      "5  5 취업          대학원생      머신러닝, 인공지능(AI), 클라우드 연구개발 경력직 채용  20-12-28   \n",
      "\n",
      "   deadline dday     sponsor                                title2  \\\n",
      "0  21-04-30   65       엘지이노텍               2021년 상반기 해외 석박사 R&D 채용   \n",
      "1  21-03-01    5    ㈜하나금융티아이          [하나금융융합기술원] 각 부문별 경력직 연구원 모집   \n",
      "2  21-03-10   14         ㈜웹젠               [전문연구요원] 빅데이터 엔지니어 수시채용   \n",
      "3  21-02-25    0    한국수력원자력㈜  2021년도 제1차 전문경력직(연구ㆍ전문ㆍ경력신입) 선발 모집요강   \n",
      "4  21-04-15   50  엔에이치엔 주식회사            [NHN] 2021년 (신입) 전문연구요원 모집   \n",
      "5  21-03-28   32        ㈜씨이랩      머신러닝, 인공지능(AI), 클라우드 연구개발 경력직 채용   \n",
      "\n",
      "                                                link  \n",
      "0  http://www.jobkorea.co.kr/Recruit/GI_Read/3398...  \n",
      "1  http://www.jobkorea.co.kr/Recruit/GI_Read/3394...  \n",
      "2  http://www.jobkorea.co.kr/Recruit/GI_Read/3393...  \n",
      "3  http://www.jobkorea.co.kr/Recruit/GI_Read/3387...  \n",
      "4  http://www.jobkorea.co.kr/Recruit/GI_Read/3366...  \n",
      "5  http://www.jobkorea.co.kr/Recruit/GI_Read/3352...  \n",
      "       type qualification                            title      notice  \\\n",
      "0    1 경진대회        대학(원)생      제3회 오픈인프라개발경진대회 (OIDC 2021)  21. 04. 19   \n",
      "1    1 경진대회        대학(원)생               스타트업과 함께하는 피버팅 해커톤  21. 02. 26   \n",
      "2    1 경진대회        대학(원)생                  2021 다솜이 드림메이커스  21. 01. 13   \n",
      "3     2 공모전        대학(원)생  제2기 LMO(유전자변형생물체) SAFETY 기자단 모집    21-02-22   \n",
      "4     2 공모전        대학(원)생                  2021 의료기기 창업공모전    21-02-23   \n",
      "..      ...           ...                              ...         ...   \n",
      "173    5 취업           대학생  웹 기획, 서비스 기획, 제안 및 컨설팅 경력 인재 초빙    21-02-22   \n",
      "174    5 취업           대학생                한세드림 전산개발팀 경력직 채용    21-02-23   \n",
      "175    5 취업           대학생                       웹기획 경력직 모집    21-02-23   \n",
      "176    5 취업           대학생               [신입/경력] 영어 편집기획 채용    21-02-24   \n",
      "177    5 취업           대학생                [신입/경력] 영어 연구원 채용    21-02-24   \n",
      "\n",
      "       deadline dday              sponsor                             title2  \\\n",
      "0    21. 08. 24    0                   맨텍        제3회 오픈인프라개발경진대회 (OIDC 2021)   \n",
      "1    21. 03. 08    0               한국무역협회                 스타트업과 함께하는 피버팅 해커톤   \n",
      "2    21. 12. 31    0     생명보험사회공헌재단, 교보생명                    2021 다솜이 드림메이커스   \n",
      "3      21-03-15    0            과학기술정보통신부  제2기 LMO(유전자변형생물체) SAFETY 기자단 모집     \n",
      "4      21-03-23    0  강원도, 원주시, 한국보건산업진흥원                   2021 의료기기 창업공모전    \n",
      "..          ...  ...                  ...                                ...   \n",
      "173    21-05-23    0               ㈜비타소프트    웹 기획, 서비스 기획, 제안 및 컨설팅 경력 인재 초빙   \n",
      "174    21-05-24    0                한세드림㈜                  한세드림 전산개발팀 경력직 채용   \n",
      "175    21-05-24    0               ㈜매그넘빈트                         웹기획 경력직 모집   \n",
      "176    21-05-25    0              ㈜해커스어학원                 [신입/경력] 영어 편집기획 채용   \n",
      "177    21-05-25    0              ㈜해커스어학원                  [신입/경력] 영어 연구원 채용   \n",
      "\n",
      "                                                  link  \n",
      "0    https://www.contestkorea.com/sub/view.php?disp...  \n",
      "1    https://www.contestkorea.com/sub/view.php?disp...  \n",
      "2    https://www.contestkorea.com/sub/view.php?disp...  \n",
      "3    https://thinkyou.co.kr/contest/sector_view.asp...  \n",
      "4    https://thinkyou.co.kr/contest/sector_view.asp...  \n",
      "..                                                 ...  \n",
      "173  http://www.jobkorea.co.kr/Recruit/GI_Read/3397...  \n",
      "174  http://www.jobkorea.co.kr/Recruit/GI_Read/3398...  \n",
      "175  http://www.jobkorea.co.kr/Recruit/GI_Read/3397...  \n",
      "176  http://www.jobkorea.co.kr/Recruit/GI_Read/3399...  \n",
      "177  http://www.jobkorea.co.kr/Recruit/GI_Read/3399...  \n",
      "\n",
      "[178 rows x 9 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>qualification</th>\n",
       "      <th>title</th>\n",
       "      <th>notice</th>\n",
       "      <th>deadline</th>\n",
       "      <th>dday</th>\n",
       "      <th>sponsor</th>\n",
       "      <th>title2</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 경진대회</td>\n",
       "      <td>대학(원)생</td>\n",
       "      <td>제3회 오픈인프라개발경진대회 (OIDC 2021)</td>\n",
       "      <td>21. 04. 19</td>\n",
       "      <td>21. 08. 24</td>\n",
       "      <td>0</td>\n",
       "      <td>맨텍</td>\n",
       "      <td>제3회 오픈인프라개발경진대회 (OIDC 2021)</td>\n",
       "      <td>https://www.contestkorea.com/sub/view.php?disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 경진대회</td>\n",
       "      <td>대학(원)생</td>\n",
       "      <td>스타트업과 함께하는 피버팅 해커톤</td>\n",
       "      <td>21. 02. 26</td>\n",
       "      <td>21. 03. 08</td>\n",
       "      <td>0</td>\n",
       "      <td>한국무역협회</td>\n",
       "      <td>스타트업과 함께하는 피버팅 해커톤</td>\n",
       "      <td>https://www.contestkorea.com/sub/view.php?disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 경진대회</td>\n",
       "      <td>대학(원)생</td>\n",
       "      <td>2021 다솜이 드림메이커스</td>\n",
       "      <td>21. 01. 13</td>\n",
       "      <td>21. 12. 31</td>\n",
       "      <td>0</td>\n",
       "      <td>생명보험사회공헌재단, 교보생명</td>\n",
       "      <td>2021 다솜이 드림메이커스</td>\n",
       "      <td>https://www.contestkorea.com/sub/view.php?disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 공모전</td>\n",
       "      <td>대학(원)생</td>\n",
       "      <td>제2기 LMO(유전자변형생물체) SAFETY 기자단 모집</td>\n",
       "      <td>21-02-22</td>\n",
       "      <td>21-03-15</td>\n",
       "      <td>0</td>\n",
       "      <td>과학기술정보통신부</td>\n",
       "      <td>제2기 LMO(유전자변형생물체) SAFETY 기자단 모집</td>\n",
       "      <td>https://thinkyou.co.kr/contest/sector_view.asp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 공모전</td>\n",
       "      <td>대학(원)생</td>\n",
       "      <td>2021 의료기기 창업공모전</td>\n",
       "      <td>21-02-23</td>\n",
       "      <td>21-03-23</td>\n",
       "      <td>0</td>\n",
       "      <td>강원도, 원주시, 한국보건산업진흥원</td>\n",
       "      <td>2021 의료기기 창업공모전</td>\n",
       "      <td>https://thinkyou.co.kr/contest/sector_view.asp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>5 취업</td>\n",
       "      <td>대학생</td>\n",
       "      <td>웹 기획, 서비스 기획, 제안 및 컨설팅 경력 인재 초빙</td>\n",
       "      <td>21-02-22</td>\n",
       "      <td>21-05-23</td>\n",
       "      <td>0</td>\n",
       "      <td>㈜비타소프트</td>\n",
       "      <td>웹 기획, 서비스 기획, 제안 및 컨설팅 경력 인재 초빙</td>\n",
       "      <td>http://www.jobkorea.co.kr/Recruit/GI_Read/3397...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>5 취업</td>\n",
       "      <td>대학생</td>\n",
       "      <td>한세드림 전산개발팀 경력직 채용</td>\n",
       "      <td>21-02-23</td>\n",
       "      <td>21-05-24</td>\n",
       "      <td>0</td>\n",
       "      <td>한세드림㈜</td>\n",
       "      <td>한세드림 전산개발팀 경력직 채용</td>\n",
       "      <td>http://www.jobkorea.co.kr/Recruit/GI_Read/3398...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>5 취업</td>\n",
       "      <td>대학생</td>\n",
       "      <td>웹기획 경력직 모집</td>\n",
       "      <td>21-02-23</td>\n",
       "      <td>21-05-24</td>\n",
       "      <td>0</td>\n",
       "      <td>㈜매그넘빈트</td>\n",
       "      <td>웹기획 경력직 모집</td>\n",
       "      <td>http://www.jobkorea.co.kr/Recruit/GI_Read/3397...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>5 취업</td>\n",
       "      <td>대학생</td>\n",
       "      <td>[신입/경력] 영어 편집기획 채용</td>\n",
       "      <td>21-02-24</td>\n",
       "      <td>21-05-25</td>\n",
       "      <td>0</td>\n",
       "      <td>㈜해커스어학원</td>\n",
       "      <td>[신입/경력] 영어 편집기획 채용</td>\n",
       "      <td>http://www.jobkorea.co.kr/Recruit/GI_Read/3399...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>5 취업</td>\n",
       "      <td>대학생</td>\n",
       "      <td>[신입/경력] 영어 연구원 채용</td>\n",
       "      <td>21-02-24</td>\n",
       "      <td>21-05-25</td>\n",
       "      <td>0</td>\n",
       "      <td>㈜해커스어학원</td>\n",
       "      <td>[신입/경력] 영어 연구원 채용</td>\n",
       "      <td>http://www.jobkorea.co.kr/Recruit/GI_Read/3399...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type qualification                            title      notice  \\\n",
       "0    1 경진대회        대학(원)생      제3회 오픈인프라개발경진대회 (OIDC 2021)  21. 04. 19   \n",
       "1    1 경진대회        대학(원)생               스타트업과 함께하는 피버팅 해커톤  21. 02. 26   \n",
       "2    1 경진대회        대학(원)생                  2021 다솜이 드림메이커스  21. 01. 13   \n",
       "3     2 공모전        대학(원)생  제2기 LMO(유전자변형생물체) SAFETY 기자단 모집    21-02-22   \n",
       "4     2 공모전        대학(원)생                  2021 의료기기 창업공모전    21-02-23   \n",
       "..      ...           ...                              ...         ...   \n",
       "173    5 취업           대학생  웹 기획, 서비스 기획, 제안 및 컨설팅 경력 인재 초빙    21-02-22   \n",
       "174    5 취업           대학생                한세드림 전산개발팀 경력직 채용    21-02-23   \n",
       "175    5 취업           대학생                       웹기획 경력직 모집    21-02-23   \n",
       "176    5 취업           대학생               [신입/경력] 영어 편집기획 채용    21-02-24   \n",
       "177    5 취업           대학생                [신입/경력] 영어 연구원 채용    21-02-24   \n",
       "\n",
       "       deadline dday              sponsor                             title2  \\\n",
       "0    21. 08. 24    0                   맨텍        제3회 오픈인프라개발경진대회 (OIDC 2021)   \n",
       "1    21. 03. 08    0               한국무역협회                 스타트업과 함께하는 피버팅 해커톤   \n",
       "2    21. 12. 31    0     생명보험사회공헌재단, 교보생명                    2021 다솜이 드림메이커스   \n",
       "3      21-03-15    0            과학기술정보통신부  제2기 LMO(유전자변형생물체) SAFETY 기자단 모집     \n",
       "4      21-03-23    0  강원도, 원주시, 한국보건산업진흥원                   2021 의료기기 창업공모전    \n",
       "..          ...  ...                  ...                                ...   \n",
       "173    21-05-23    0               ㈜비타소프트    웹 기획, 서비스 기획, 제안 및 컨설팅 경력 인재 초빙   \n",
       "174    21-05-24    0                한세드림㈜                  한세드림 전산개발팀 경력직 채용   \n",
       "175    21-05-24    0               ㈜매그넘빈트                         웹기획 경력직 모집   \n",
       "176    21-05-25    0              ㈜해커스어학원                 [신입/경력] 영어 편집기획 채용   \n",
       "177    21-05-25    0              ㈜해커스어학원                  [신입/경력] 영어 연구원 채용   \n",
       "\n",
       "                                                  link  \n",
       "0    https://www.contestkorea.com/sub/view.php?disp...  \n",
       "1    https://www.contestkorea.com/sub/view.php?disp...  \n",
       "2    https://www.contestkorea.com/sub/view.php?disp...  \n",
       "3    https://thinkyou.co.kr/contest/sector_view.asp...  \n",
       "4    https://thinkyou.co.kr/contest/sector_view.asp...  \n",
       "..                                                 ...  \n",
       "173  http://www.jobkorea.co.kr/Recruit/GI_Read/3397...  \n",
       "174  http://www.jobkorea.co.kr/Recruit/GI_Read/3398...  \n",
       "175  http://www.jobkorea.co.kr/Recruit/GI_Read/3397...  \n",
       "176  http://www.jobkorea.co.kr/Recruit/GI_Read/3399...  \n",
       "177  http://www.jobkorea.co.kr/Recruit/GI_Read/3399...  \n",
       "\n",
       "[178 rows x 9 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sum()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "retired-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tofb():\n",
    "    fin = df#sum()\n",
    "\n",
    "\n",
    "    postdata = fin.to_dict(orient=\"index\")\n",
    "    # 파이어베이스 Config 정보\n",
    "    config = {\n",
    "        \"apiKey\": \"AIzaSyDIo8bt7OrCX6KYaxplvUauQdaehcjUo_0\",\n",
    "        \"authDomain\": \"activity-crawling.firebaseapp.com\",\n",
    "        \"databaseURL\": \"https://activity-crawling-default-rtdb.firebaseio.com\",\n",
    "        \"projectId\": \"activity-crawling\",\n",
    "        \"storageBucket\": \"activity-crawling.appspot.com\",\n",
    "        \"messagingSenderId\": \"608978503357\",\n",
    "        \"appId\": \"1:608978503357:web:374a269b8fa1a64888d9d4\"}\n",
    "    firebase = pyrebase.initialize_app(config)\n",
    "    db = firebase.database()\n",
    "    # 파이어베이스 DB 내용 제거 후 업데이트\n",
    "    db.remove()\n",
    "    db.child().update(postdata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "caroline-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "tofb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job():\n",
    "    now = datetime.now()\n",
    "    print(now)\n",
    "    tofb()\n",
    "    print(\"end\")\n",
    "\n",
    "\n",
    "schedule.every().day.at(\"00:00\").do(job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-lending",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
