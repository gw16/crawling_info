{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#크롤링을 위한 라이브러리\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "from selenium.webdriver import Chrome   \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "import MySQLdb\n",
    "import mysql.connector\n",
    "\n",
    "import schedule\n",
    "import time\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser():\n",
    "    # 가져올 사이트 페이지 URL\n",
    "    url ='https://thinkyou.co.kr/contest/sector.asp'\n",
    "    # 크롬 드라이버 위치 지정 (상대경로 아니니까 각자 PC에 맞게 알아서 경로 잡아랏)\n",
    "    browser = Chrome(r\"C:\\Users\\user\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    # 모든 자원이 로드될때까지 기다리게하는 시간 지정 (3초)\n",
    "    delay=3\n",
    "    browser.implicitly_wait(delay)\n",
    "    # get()을 통해 해당 URL에 접근\n",
    "    browser.get(url) \n",
    "    # 크롬 브라우저 창 크기 조절\n",
    "    browser.maximize_window()\n",
    "    # tag name 통해 element에 접근\n",
    "    body = browser.find_element_by_tag_name('body')\n",
    "    # 과학/공학 카테고리 글만 보기\n",
    "    try :\n",
    "    # xpath 통해 element 가져오기\n",
    "    # F12하고 과학/공학 버튼 위치 간다음 우클릭 copy 하면 copy xpath 뜸\n",
    "    # 해당 xpath : //*[@id=\"searchFrm\"]/div/dl[1]/dd/p[6]/label/span\n",
    "        browser.find_elements_by_xpath('//*[@id=\"searchFrm\"]/div/dl[1]/dd/p[6]/label/span')[0].click()\n",
    "        browser.find_elements_by_xpath('//*[@id=\"searchFrm\"]/div/dl[2]/dd/p[1]/label/span')[0].click()\n",
    "    except:\n",
    "        pass\n",
    "    # 웹페이지 스크롤\n",
    "    # https://cnpnote.tistory.com/entry/PYTHON-%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%97%90%EC%84%9C-selenium-webdriver%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EC%97%AC-%EC%9B%B9-%ED%8E%98%EC%9D%B4%EC%A7%80%EB%A5%BC-%EC%8A%A4%ED%81%AC%EB%A1%A4%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95%EC%9D%80-%EB%AC%B4%EC%97%87%EC%9E%85%EB%8B%88%EA%B9%8C\n",
    "    # 실행하고 나면 해당 페이지 스크롤 맨아래까지 내려감\n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "    while True:\n",
    "        last_height = browser.execute_script('return document.documentElement.scrollHeight')\n",
    "    # 현재 화면의 길이를 리턴 받아 last_height에 넣음\n",
    "        for i in range(3):\n",
    "            body.send_keys(Keys.END)\n",
    "        # body 본문에 END키를 입력(스크롤내림)\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = browser.execute_script('return document.documentElement.scrollHeight')\n",
    "        if new_height == last_height:\n",
    "            break;\n",
    "    # 페이지 소스 리턴\n",
    "    # https://beomi.github.io/2017/02/27/HowToMakeWebCrawler-With-Selenium/\n",
    "    page = browser.page_source #페이지 Element모두 가져옴\n",
    "    soup = BeautifulSoup(page, 'lxml') # Beautiful Soup 사용하기\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling():\n",
    "    soup = browser()\n",
    "    # 가져올 공모전 개수는 그냥 20개로 함\n",
    "    len_day = 20\n",
    "    # 마감 전\n",
    "    links_bef = [] # 링크\n",
    "    titles_bef = [] # 행사 이름\n",
    "    dday_bef = [] # D-DAY\n",
    "    inst_bef = [] # 주최 측\n",
    "    start_bef = []\n",
    "    end_bef =[]\n",
    "    # 마감 후\n",
    "    links_aft = [] # 링크\n",
    "    titles_aft = [] # 행사 이름\n",
    "    inst_aft = [] # 주최 측\n",
    "    \n",
    "    \n",
    "    for i in range(len_day):\n",
    "        t = soup.select(' .title > a > dl > dt ')[i].text\n",
    "        fin = soup.select(' .statNew > p ')[i].text\n",
    "    \n",
    "        # 마감된 공모전\n",
    "        if fin  == '마감':\n",
    "            base_url = 'https://thinkyou.co.kr'\n",
    "            titles_aft.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "            inst_aft.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "            #dday_aft.append('D' + soup.select(' .statNew')[i].text.split('D')[1])\n",
    "            links_aft.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "        else :\n",
    "            stand = soup.select(' .statNew')[i].text.split('D')[1]\n",
    "        \n",
    "            # 해당 D-DAY\n",
    "            if stand == '-day':\n",
    "                base_url = 'https://thinkyou.co.kr'\n",
    "                titles_bef.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "                inst_bef.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "                dday_bef.append('D' + soup.select(' .statNew')[i].text.split('D')[1])\n",
    "                links_bef.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "                a = i*2\n",
    "                start_bef.append(soup.select(' .etc')[a].text[:8])\n",
    "                end_bef.append(soup.select(' .etc')[a].text[11:])\n",
    "            \n",
    "            else :\n",
    "                # 마감 전\n",
    "                base_url = 'https://thinkyou.co.kr'\n",
    "                titles_bef.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "                inst_bef.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "                dday_bef.append('D' + soup.select(' .statNew')[i].text.split('D')[1])\n",
    "                links_bef.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "                a = i*2\n",
    "                start_bef.append(soup.select(' .etc')[a].text[:8])\n",
    "                end_bef.append(soup.select(' .etc')[a].text[11:])\n",
    "\n",
    "    # 마감 전\n",
    "    tabl_data_bef = {'제목': titles_bef, '공지일': start_bef, '마감일': end_bef, '날짜': dday_bef,'기관':inst_bef, '링크':links_bef}\n",
    "    df_bef = pd.DataFrame(tabl_data_bef, columns=['제목','공지일', '마감일','날짜','기관','링크'])\n",
    "    df_bef['분류'] = '공모전'\n",
    "    df_bef['자격'] = '대학(원)생'\n",
    "    #df_bef.to_csv('thinkyou_bef.csv', encoding='utf-8-sig')\n",
    "    # 마감 후\n",
    "    tabl_data_aft = {'제목': titles_aft, '기관':inst_aft,'링크':links_aft}\n",
    "    df_aft = pd.DataFrame(tabl_data_aft, columns=['제목','날짜','링크'])  \n",
    "    df_aft['날짜'] = '마감'\n",
    "    #df_aft.to_csv('thinkyou_aft.csv', encoding='utf-8-sig')\n",
    "    \n",
    "    return df_bef\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tosql():\n",
    "    df = crawling()\n",
    "    engine = create_engine(\"mysql+pymysql://root:x03md2$9c@localhost:3306/CRAW_TABLE\", encoding='utf-8', echo=False)\n",
    "    \n",
    "    df.to_sql(name='thinkyou', con=engine, if_exists= 'replace', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-05 21:44:04.932745\n",
      "2021-02-05 21:45:14.480416\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    now = datetime.now()\n",
    "    print(now)\n",
    "    tosql()\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
