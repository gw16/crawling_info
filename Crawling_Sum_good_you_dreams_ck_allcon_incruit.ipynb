{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "smooth-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import db\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "controlled-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "from time import sleep\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "import schedule\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "experimental-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "context=ssl._create_unverified_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "suspended-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser():\n",
    "\n",
    "    url ='https://thinkyou.co.kr/contest/sector.asp'\n",
    "\n",
    "    browser = Chrome('./chromedriver')\n",
    "\n",
    "    delay=3\n",
    "    browser.implicitly_wait(delay)\n",
    "\n",
    "    browser.get(url)\n",
    "\n",
    "    browser.maximize_window()\n",
    "\n",
    "    body = browser.find_element_by_tag_name('body')\n",
    "\n",
    "    try :\n",
    "\n",
    "        browser.find_elements_by_xpath('//*[@id=\"searchFrm\"]/div/dl[1]/dd/p[6]/label/span')[0].click()\n",
    "        browser.find_elements_by_xpath('//*[@id=\"searchFrm\"]/div/dl[2]/dd/p[1]/label/span')[0].click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "    while True:\n",
    "        last_height = browser.execute_script('return document.documentElement.scrollHeight')\n",
    "\n",
    "        for i in range(3):\n",
    "            body.send_keys(Keys.END)\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = browser.execute_script('return document.documentElement.scrollHeight')\n",
    "        if new_height == last_height:\n",
    "            break;\n",
    "\n",
    "    page = browser.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stylish-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling():\n",
    "    soup = browser()\n",
    "\n",
    "    len_day = 20\n",
    "\n",
    "    links_bef = []\n",
    "    titles_bef = []\n",
    "    dday_bef = []\n",
    "    inst_bef = []\n",
    "    start_bef = []\n",
    "    end_bef = []\n",
    "\n",
    "    links_aft = []\n",
    "    titles_aft = []\n",
    "    inst_aft = []\n",
    "\n",
    "    for i in range(len_day):\n",
    "        t = soup.select(' .title > a > dl > dt ')[i].text\n",
    "        fin = soup.select(' .statNew > p ')[i].text\n",
    "\n",
    "\n",
    "        if fin == '마감':\n",
    "            base_url = 'https://thinkyou.co.kr'\n",
    "            titles_aft.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "            inst_aft.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "\n",
    "            links_aft.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "        else:\n",
    "            stand = soup.select(' .statNew')[i].text.split('D')[1]\n",
    "\n",
    "\n",
    "            if stand == '-day':\n",
    "                num = 0\n",
    "                base_url = 'https://thinkyou.co.kr'\n",
    "                titles_bef.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "                inst_bef.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "                dday_bef.append(num)\n",
    "                links_bef.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "                a = i * 2\n",
    "                start_bef.append(soup.select(' .etc')[a].text[:8])\n",
    "                end_bef.append(soup.select(' .etc')[a].text[11:])\n",
    "\n",
    "            else:\n",
    "\n",
    "                base_url = 'https://thinkyou.co.kr'\n",
    "                titles_bef.append(soup.select(' .title > a > dl > dt ')[i].text)\n",
    "                inst_bef.append(soup.select(' .title > a > dl > dd ')[i].text.split(':')[1][1:])\n",
    "                dday_bef.append(soup.select(' .statNew')[i].text.split('-')[1])\n",
    "                links_bef.append(base_url + soup.select(' .title > a')[i]['href'][2:])\n",
    "                a = i * 2\n",
    "                start_bef.append(soup.select(' .etc')[a].text[:8])\n",
    "                end_bef.append(soup.select(' .etc')[a].text[11:])\n",
    "                \n",
    "    print(inst_bef)\n",
    "\n",
    "    tabl_data_bef = {'title': titles_bef, 'notice': start_bef, 'deadline': end_bef, \n",
    "                     'dday': dday_bef, 'sponsor': inst_bef, 'title2': titles_bef, 'link': links_bef}\n",
    "    print(tabl_data_bef)\n",
    "\n",
    "    df_bef = pd.DataFrame(tabl_data_bef, columns=['type', 'qualification', 'title', \n",
    "                                                  'notice', 'deadline', 'dday', 'sponsor', 'title2', 'link'])\n",
    "    \n",
    "    df_bef['type'] = '공모전'\n",
    "    df_bef['qualification'] = '대학(원)생'\n",
    "\n",
    "\n",
    "    return df_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "foreign-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser2():\n",
    "    url_base = 'https://www.thinkcontest.com/Contest/CateField.html?page=1&c=11'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url_base, headers=headers)\n",
    "    soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "    key = ['과학/공학', '게임/소프트웨어']\n",
    "    links = []\n",
    "    titles = []\n",
    "    dday = []\n",
    "    inst = []\n",
    "    dates = []\n",
    "    k = 1\n",
    "    \n",
    "    while k <= 10:\n",
    "        url = 'https://www.thinkcontest.com/Contest/CateField.html?page=' + str(k) + '&c=11'\n",
    "        base_url = 'https://www.thinkcontest.com/'\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        len_link = len(soup.select(' .txt-left > .contest-title > a'))\n",
    "        for i in range(len_link):\n",
    "            if soup.select(' td > span ')[i].text.replace('\\n', '') == '마감':\n",
    "                break\n",
    "            else:\n",
    "                titles.append(soup.select(' .txt-left > .contest-title > a')[i].text)\n",
    "                links.append(base_url + soup.select('.txt-left > .contest-title > a ')[i]['href'])\n",
    "                dday.append(soup.select(' td > p ')[i].text.split('-')[1])\n",
    "        k=k+1\n",
    "                            \n",
    "    str_date = []\n",
    "    end_date = []\n",
    "    participate = []\n",
    "    for i in range(len(links)):\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(links[i], headers=headers)\n",
    "        soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "        html = soup.select(' tr')\n",
    "        text = str(html).replace('\\n', '')\n",
    "        certi = re.compile('참가자격' + '.{200}')\n",
    "        test = certi.findall(text)[0]\n",
    "        partis = []\n",
    "        if '대학생' in test:\n",
    "            partis.append('대학생')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if '대학원생' in test:\n",
    "            partis.append('대학원생')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if '일반인' in test:\n",
    "            partis.append('일반인')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if '국내외 석학과 연구진' in test:\n",
    "            partis.append('국내외 석학과 연구진')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if '제한없음' in test:\n",
    "            partis.append('제한없음')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if '어린이' in test:\n",
    "            partis.append('어린이')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if '초등학생' in test:\n",
    "            partis.append('초등학생')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if '중학생' in test:\n",
    "            partis.append('중학생')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if '고등학생' in test:\n",
    "            partis.append('고등학생')\n",
    "        else:\n",
    "            pass\n",
    "        participant = str(partis).replace('[', '').replace(']', '').replace(\"'\", \"\")\n",
    "        start = re.compile('접수기간' + '.{19}')\n",
    "        strdate = start.findall(text)[0].split('<td>')[1]\n",
    "        end = re.compile('접수기간' + '.{32}')\n",
    "        enddate = end.findall(text)[0].split('~')[1].replace(' ', '')\n",
    "        participate.append(participant)\n",
    "        str_date.append(strdate)\n",
    "        end_date.append(enddate)\n",
    "        inst.append(soup.select(' tbody > tr > td ')[0].text)\n",
    "        \n",
    "        \n",
    "\n",
    "    tabl_data = {'title': titles, 'notice': str_date, 'deadline': end_date, 'dday': dday,\n",
    "                 'qualification': participate, 'sponsor': inst, 'title2': titles,'link': links}\n",
    "\n",
    "    df2 = pd.DataFrame(tabl_data, columns=['type', 'qualification', 'title', \n",
    "                                           'notice', 'deadline', 'dday', 'sponsor', 'title2', 'link'])\n",
    "    df2['type'] = '공모전'\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sealed-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_dreams():\n",
    "    dday_bef = []\n",
    "    page_num = 1\n",
    "    while(page_num <=5):        \n",
    "        url = 'https://www.dreamspon.com/scholarship/scholarship02.html?page=' + str(page_num)\n",
    "        req = urllib.request.urlopen(url)\n",
    "        res = req.read()\n",
    "        soup = BeautifulSoup(res,'html.parser')\n",
    "        days = soup.select(\" .td_day > .count\")        \n",
    "        for i in range(len(days)):\n",
    "            if 'D+' in str(days[i].text):\n",
    "                pass\n",
    "            else:\n",
    "                \n",
    "                dday_bef.append((days[i].text).strip(\"D-\"))\n",
    "        page_num += 1  \n",
    "    return dday_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "valuable-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_test_dreams():\n",
    "    link_test = []\n",
    "    page_num = 1\n",
    "    while(page_num <=5):        \n",
    "        url = 'https://www.dreamspon.com/scholarship/scholarship02.html?page=' + str(page_num)\n",
    "        req = urllib.request.urlopen(url)\n",
    "        res = req.read()\n",
    "        soup = BeautifulSoup(res,'html.parser')\n",
    "        contests = soup.find_all(\"p\",class_=\"title\")\n",
    "        days = soup.select(\" .td_day > .count\")        \n",
    "        for i in range(len(days)):\n",
    "            if 'D+' in str(days[i].text):\n",
    "                pass\n",
    "            else:\n",
    "                link_test.append(str(contests[i]).strip('[<p class=\"title\"><a href=\"').strip('</a>'))\n",
    "        page_num += 1  \n",
    "    return link_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sunset-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_dreams(link_test):\n",
    "    link_bef=[]\n",
    "    page_num = 1\n",
    "    for t in range(len(link_test)):\n",
    "        link_address, title_name = link_test[t].split('\">')\n",
    "        link_ver1 = \"https://www.dreamspon.com/\" + link_address\n",
    "        link_bef.append(link_ver1)\n",
    "    page_num += 1  \n",
    "    return link_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "representative-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def titles_dreams(link_test):\n",
    "    titles_bef = [] # 행사 이름\n",
    "    page_num = 1\n",
    "    for t in range(len(link_test)):\n",
    "        link_address, title_name = link_test[t].split('\">')\n",
    "        titles_bef.append(title_name)\n",
    "    page_num += 1  \n",
    "    return titles_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "digital-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insts_dreams():\n",
    "    inst = []\n",
    "    page_num = 1\n",
    "    while(page_num <=5):\n",
    "        url = 'https://www.dreamspon.com/scholarship/scholarship02.html?page=' + str(page_num)\n",
    "        req = urllib.request.urlopen(url)\n",
    "        res = req.read()\n",
    "        soup = BeautifulSoup(res,'html.parser')\n",
    "        idx = 1\n",
    "        while(idx<=60):\n",
    "            if 'D-' in str(soup.select(\"tr>td\")[idx+1].text):\n",
    "                inst.append(soup.select(\"tr>td\")[idx].text) #       1,5,9, 13\n",
    "            idx += 4    \n",
    "        page_num += 1\n",
    "    return inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "appropriate-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_e_test_dreams(list_adress):\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver')\n",
    "    driver.implicitly_wait(10)\n",
    "    login_path ='//*[@id=\"loginForm\"]/div[1]/input'\n",
    "\n",
    "    driver.get(\"https://www.dreamspon.com/\" + list_adress)\n",
    "    result = driver.switch_to_alert()\n",
    "    result.accept()\n",
    "\n",
    "    driver.find_element_by_name('mbr_id').send_keys('rainrain16@hanmail.net')\n",
    "    driver.find_element_by_name('pwd_in').send_keys('rainrain16')\n",
    "    driver.find_element_by_xpath(login_path).click()\n",
    "    #   response = driver.get(\"https://www.dreamspon.com/\" + link_adress)\n",
    "    sleep(2)\n",
    "\n",
    "    html = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "    crawl_data =soup.select(\"dl >dd >p>b\")[0].text\n",
    "\n",
    "\n",
    "    start_day, end_day  = (crawl_data).split('~')\n",
    "\n",
    "\n",
    "\n",
    "    return start_day, end_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "former-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_e_test_dreams(list_adress):\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver')\n",
    "    driver.implicitly_wait(10)\n",
    "    login_path ='//*[@id=\"loginForm\"]/div[1]/input'\n",
    "\n",
    "    driver.get(list_adress)\n",
    "    result = driver.switch_to_alert()\n",
    "    result.accept()\n",
    "\n",
    "    driver.find_element_by_name('mbr_id').send_keys('rainrain16@hanmail.net')\n",
    "    driver.find_element_by_name('pwd_in').send_keys('rainrain16')\n",
    "    driver.find_element_by_xpath(login_path).click()\n",
    "\n",
    "    sleep(1)\n",
    "\n",
    "    html = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "    crawl_data = str(soup.find_all(\"li\", class_= \"day\"))\n",
    "\n",
    "    return crawl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "effective-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_e_days_dreams():\n",
    "    crawl=[]\n",
    "    sd_days_list = link_dreams(link_test_dreams())\n",
    "    \n",
    "    for i in range(len(sd_days_list)):\n",
    "        crawl.append(s_e_test_dreams(sd_days_list[i]))\n",
    "    return crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "flying-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_e_preprocess():\n",
    "    s_e_pre = s_e_days_dreams()\n",
    "    \n",
    "    for i in range(len(s_e_pre)):\n",
    "        s_e_pre[i] = (s_e_pre[i].strip('[<li class=\"day\" style=\"height: 70px; \">')).strip('\\n\\t')\n",
    "\n",
    "    return s_e_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "arranged-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_e_final():\n",
    "    s_e_pre = s_e_preprocess()\n",
    "    start_bef = []\n",
    "    end_bef = []\n",
    "    for i in range(len(s_e_pre)):\n",
    "        if '(1차)' not in s_e_pre[i]:\n",
    "            start_day, end_day_ver1  = (s_e_pre[i]).split('~')\n",
    "            end_day_ver2, end_day_ver3 =  (end_day_ver1).split(\"<span>D\")\n",
    "            start_bef.append(start_day.strip('.'))\n",
    "            end_bef.append(end_day_ver2.strip('.'))\n",
    "\n",
    "        else:            \n",
    "            start_ver1, start_ver2 = (s_e_pre[i]).split('</span><br/>')\n",
    "            start_ver3, end_ver1 = start_ver1.split('~')\n",
    "            end_ver2, end_ver3 = end_ver1.split('<span')\n",
    "\n",
    "            start_ver4 ,end_ver4  = start_ver2.split('~')\n",
    "\n",
    "            \n",
    "            end_ver5, end_ver6 = end_ver4.split('<span')\n",
    "\n",
    "            \n",
    "            start_bef.append('2' + start_ver4.strip(\"(2차) \").strip(\". \"))\n",
    "            end_bef.append(end_ver5.strip(\".\").strip(\" \"))\n",
    "\n",
    "\n",
    "    return start_bef, end_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "educational-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_dreams():\n",
    "    dday =  days_dreams()\n",
    "    links = link_dreams(link_test_dreams())\n",
    "    title = titles_dreams(link_test_dreams())\n",
    "    start, end = s_e_final()\n",
    "    inst = insts_dreams()\n",
    "\n",
    "#     tabl_data_bef = {'title': title, 'notice': start,  'deadline': end, 'dday': dday, 'sponsor': inst, 'title2': title,\n",
    "#                      'link': links}\n",
    "#     df_bef = pd.DataFrame(tabl_data_bef, columns=['title', 'notice', 'deadline', 'dday', 'sponsor', 'title2', 'link'])\n",
    "#     df_bef['type'] = '장학금'\n",
    "#     df_bef['qualification'] = '대학생'\n",
    "    \n",
    "    tabl_data_bef = {'type':\"장학금\", 'qualification': \"대학생\",'title': title, 'notice': start,  'deadline': end, 'dday':dday, 'sponsor':inst, 'title2': title,\n",
    "                     'link':links}\n",
    "#     df_bef = pd.DataFrame()\n",
    "#     df_bef['type'] = '공모전'\n",
    "#     df_bef['qualification'] = '대학(원)생'\n",
    "    df_bef = pd.DataFrame(tabl_data_bef, columns=['type', 'qualification', 'title', 'notice', 'deadline', 'dday', 'sponsor', 'title2', 'link'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return df_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-fraud",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-symposium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "assigned-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_ck():\n",
    "    title_bef =[]\n",
    "    days_bef = []\n",
    "    start_bef = []\n",
    "    end_bef = []\n",
    "    conditions = []\n",
    "    inst_bef = []\n",
    "    link_bef = []\n",
    "    page_num = 1\n",
    "    while(page_num<=5):\n",
    "        url = 'https://www.contestkorea.com/sub/list.php?displayrow=12&Txt_sGn=1&Txt_key=all&Txt_word=&Txt_bcode=030210001&Txt_code1%5B0%5D=30&Txt_code1%5B1%5D=76&Txt_aarea=&Txt_area=&Txt_sortkey=a.int_sort&Txt_sortword=desc&Txt_chocode=&Txt_unicode=&page=' + str(page_num)\n",
    "        req = urllib.request.urlopen(url)\n",
    "        res = req.read()\n",
    "        soup = BeautifulSoup(res,'html.parser')\n",
    "        titles = soup.find_all(\"span\", class_ = \"txt\")\n",
    "        titles = list(titles)\n",
    "        condition = soup.find_all(\"span\", class_=\"condition\")\n",
    "        condition = list(condition)\n",
    "        days = soup.select(\".date > div\" )\n",
    "        days = list(days)\n",
    "        \n",
    "        inst = soup.select(\".host > .icon_1\" )\n",
    "        inst = list(inst)\n",
    "\n",
    "        for i in range(len(condition)):\n",
    "            days[i] = (str(days[i]).strip('<div>').strip(\">\").strip(\"\\t\"))\n",
    "            condition[i] = (str(condition[i]).strip('<span class=\"condition\">').strip(\"</span>\"))\n",
    "            if condition[i] != '접수종료':\n",
    "                conditions.append((condition[i]))\n",
    "                titles[i] = (str(titles[i]).strip('<span class=\"txt\">').strip(\"</'\").strip(\" \"))\n",
    "                days[i] = (str(days[i]).strip('<div>').strip('\\n\\t').strip('</li>'))\n",
    "                inst[i] = (str(inst[i]).strip('<li class=\"icon_1\"><strong>주최</strong> . ').strip('<div>'))\n",
    "                if '공모전' not in titles[i]:\n",
    "                    title_bef.append(titles[i])\n",
    "                    start_day,end_day = days[i].split('~')\n",
    "                    start_bef.append(start_day.replace(\".\", \". \"))\n",
    "                    end_bef.append(end_day.replace(\".\", \". \").strip('\\t'))\n",
    "                    inst_bef.append(inst[i])             \n",
    "        page_num+=1\n",
    "    return title_bef, start_bef, end_bef, inst_bef        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "sudden-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_ck():\n",
    "    conditions=[]\n",
    "    link_bef = []\n",
    "    page_num = 1\n",
    "    while(page_num<=5):\n",
    "        url = 'https://www.contestkorea.com/sub/list.php?displayrow=12&Txt_sGn=1&Txt_key=all&Txt_word=&Txt_bcode=030210001&Txt_code1%5B0%5D=30&Txt_code1%5B1%5D=76&Txt_aarea=&Txt_area=&Txt_sortkey=a.int_sort&Txt_sortword=desc&Txt_chocode=&Txt_unicode=&page=' + str(page_num)\n",
    "        req = urllib.request.urlopen(url)\n",
    "        res = req.read()\n",
    "        soup = BeautifulSoup(res,'html.parser')\n",
    "        titles = soup.find_all(\"span\", class_ = \"txt\")\n",
    "        titles = list(titles)\n",
    "        condition = soup.find_all(\"span\", class_=\"condition\")\n",
    "        condition = list(condition)\n",
    "        for i in range(len(condition)):\n",
    "            condition[i] = (str(condition[i]).strip('<span class=\"condition\">').strip(\"</span>\"))\n",
    "            if condition[i] != '접수종료':\n",
    "                conditions.append((condition[i]))\n",
    "                titles[i] = (str(titles[i]).strip('<span class=\"txt\">').strip(\"</'\").strip(\" \"))\n",
    "                if '공모전' not in titles[i]:                 \n",
    "                    browser = Chrome('./chromedriver')\n",
    "                    delay=1\n",
    "                    browser.implicitly_wait(delay)\n",
    "                    browser.get(url)\n",
    "                    browser.maximize_window()\n",
    "                    link_adress = '//*[@id=\"frm\"]/div/div[4]/ul/li['+ str(i+1) + ']/div[1]/a'\n",
    "                    browser.find_elements_by_xpath(link_adress)[0].click()\n",
    "                    sleep(0.5)\n",
    "                    link_bef.append(browser.current_url)\n",
    "                    \n",
    "                    \n",
    "        page_num+=1\n",
    "    return link_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "harmful-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_ck():\n",
    "    title_bef, start_bef, end_bef, inst_bef = base_ck()\n",
    "    link_bef = link_ck()\n",
    "    \n",
    "    tabl_data_bef = {'type':\"경진대회\", 'qualification': \"대학(원)생\",'title':  title_bef, 'notice': start_bef,  'deadline': end_bef, 'dday':0,'sponsor':inst_bef, 'title2': title_bef,\n",
    "                     'link':link_bef}\n",
    "\n",
    "    df_bef = pd.DataFrame(tabl_data_bef, columns=['type', 'qualification', 'title', 'notice', 'deadline', 'dday', 'sponsor', 'title2', 'link'])\n",
    "    return df_bef\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "secure-ancient",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>qualification</th>\n",
       "      <th>title</th>\n",
       "      <th>notice</th>\n",
       "      <th>deadline</th>\n",
       "      <th>dday</th>\n",
       "      <th>sponsor</th>\n",
       "      <th>title2</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>경진대회</td>\n",
       "      <td>대학(원)생</td>\n",
       "      <td>스타트업과 함께하는 피버팅 해커톤</td>\n",
       "      <td>21. 02. 26</td>\n",
       "      <td>21. 03. 08</td>\n",
       "      <td>0</td>\n",
       "      <td>한국무역협회</td>\n",
       "      <td>스타트업과 함께하는 피버팅 해커톤</td>\n",
       "      <td>https://www.contestkorea.com/sub/view.php?disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>경진대회</td>\n",
       "      <td>대학(원)생</td>\n",
       "      <td>2021 다솜이 드림메이커스</td>\n",
       "      <td>21. 01. 13</td>\n",
       "      <td>21. 12. 31</td>\n",
       "      <td>0</td>\n",
       "      <td>생명보험사회공헌재단, 교보생명</td>\n",
       "      <td>2021 다솜이 드림메이커스</td>\n",
       "      <td>https://www.contestkorea.com/sub/view.php?disp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type qualification               title      notice    deadline  dday  \\\n",
       "0  경진대회        대학(원)생  스타트업과 함께하는 피버팅 해커톤  21. 02. 26  21. 03. 08     0   \n",
       "1  경진대회        대학(원)생     2021 다솜이 드림메이커스  21. 01. 13  21. 12. 31     0   \n",
       "\n",
       "            sponsor              title2  \\\n",
       "0            한국무역협회  스타트업과 함께하는 피버팅 해커톤   \n",
       "1  생명보험사회공헌재단, 교보생명     2021 다솜이 드림메이커스   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.contestkorea.com/sub/view.php?disp...  \n",
       "1  https://www.contestkorea.com/sub/view.php?disp...  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ck = crawling_ck()\n",
    "test_ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blind-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_allcon():\n",
    "    a_title=[]\n",
    "    a_host=[]\n",
    "    a_terms=[]\n",
    "    a_start_bef=[]\n",
    "    a_end_bef=[]\n",
    "    a_qualification=[]\n",
    "    a_links=[]\n",
    "    a_real_links=[]\n",
    "    url2='https://www.all-con.co.kr/page'\n",
    "    for n in range(1,6):\n",
    "        base_url='https://www.all-con.co.kr/page/uni_activity.php?sc=0&st=2&sstt=&page={}'.format(n)\n",
    "        flag = False\n",
    "        url = base_url.format(n)\n",
    "        webpage = urlopen(url,context=context)\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        for i in range(1,16):\n",
    "            a_title.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > a > p')[0].get_text())\n",
    "            a_host.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td:nth-child(3) > p')[0].get_text())\n",
    "            a_terms.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > ul > li:nth-child(1) > p.info > span')[0].get_text())\n",
    "            a_qualification.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > ul > li:nth-child(2) > p.info')[0].get_text())\n",
    "            a_links.append(soup.select('#page_board_contents > div > table > tbody > tr:nth-child('+str(i)+') > td.name > a')[0].get('href').lstrip('.'))\n",
    "    return a_title, a_host,a_terms, a_qualification,a_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "earned-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allcon_days():\n",
    "    a_start_bef=[]\n",
    "    a_end_bef=[]\n",
    "    for a_term in a_terms:\n",
    "        a_start_day,a_end_day=a_term.split(\" ~ \")\n",
    "        a_start_bef.append(a_start_day)\n",
    "        a_end_bef.append(a_end_day)\n",
    "        \n",
    "    return a_start_bef, a_end_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iraqi-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_allcon():\n",
    "    a_title, a_host,a_terms ,a_qualification,a_links = parse_allcon()\n",
    "    a_real_links = allcon_real_link()\n",
    "    a_start_bef,a_end_bef=allcon_days()\n",
    "    allcon_table_data_bef = {'type':\"대외활동\", 'qualification': a_qualification,'title': a_title, 'notice': a_start_bef,  'deadline': a_end_bef, 'sponsor':a_host, 'title2': a_title,'link':a_real_links}\n",
    "    df_allcon=pd.DataFrame(allcon_table_data_bef, columns=['type', 'qualification', 'title', 'notice', 'deadline', 'sponsor', 'title2', 'link'])\n",
    "    return df_allcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "honest-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_incruit():\n",
    "    inc_title=[]\n",
    "    inc_host=[]\n",
    "    inc_terms=[]\n",
    "    inc_start_bef=[]\n",
    "    inc_end_bef=[]\n",
    "    inc_qualification=[]\n",
    "    inc_links=[]\n",
    "    inc_real_links=[]\n",
    "    base_url='https://gongmo.incruit.com/list/gongmolist.asp?ct=1&category=11'\n",
    "    webpage = urlopen(base_url,context=context)\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    for i in range(1,4):\n",
    "        inc_title.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.gmtitle > ul > a')[0].get_text())\n",
    "        inc_host.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.company')[0].get_text().lstrip('\\r\\n\\t\\t\\t\\t\\t\\t\\t').strip('\\r\\n\\t\\t\\t\\t\\t\\t\\t'))\n",
    "        inc_terms.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.due')[0].get_text())\n",
    "        inc_links.append(soup.select('#tbdyGmScrap > tr:nth-child('+str(i)+') > td.gmtitle > ul > a')[0].get('href'))\n",
    "        \n",
    "    return inc_title, inc_host, inc_terms, inc_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "peaceful-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incruit_days():\n",
    "    inc_title, inc_host, inc_terms, inc_links=parse_incruit()\n",
    "    inc_start_bef=[]\n",
    "    inc_end_bef=[]\n",
    "    for inc_term in inc_terms:\n",
    "        inc_start_day,inc_end_day=inc_term.split(\"~\")\n",
    "        inc_start_bef.append(inc_start_day)\n",
    "        inc_end_bef.append(inc_end_day)\n",
    "    return inc_start_bef, inc_end_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "convinced-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_incruit():\n",
    "    inc_title, inc_host, inc_terms, inc_links=parse_incruit()\n",
    "    inc_start_bef, inc_end_bef=incruit_days()\n",
    "    incruit_table_bef = {'type':\"공모전\", 'qualification': '대학(원)생','title': inc_title, 'notice': inc_start_bef,  'deadline': inc_end_bef, 'sponsor':inc_host, 'title2': inc_title,'link':inc_links}\n",
    "    df_incruit=pd.DataFrame(incruit_table_bef, columns=['type', 'qualification', 'title', 'notice', 'deadline', 'sponsor', 'title2', 'link'])\n",
    "    return df_incruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "uniform-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tofb():\n",
    "    df1 = crawling()\n",
    "    df2 =  final_dreams()\n",
    "    df3 = browser2()\n",
    "    df4 = crawling_ck()\n",
    "    df5= final_allcon()\n",
    "    df6= final_incruit()\n",
    "\n",
    "    mid = pd.concat([df1, df2,df3,df4,df5,df6]) \n",
    "    mid_df = mid.reset_index(drop=True)\n",
    "    mid_df['title'] = mid_df['title'].str.strip()\n",
    "    fin_df = mid_df.drop_duplicates(['title'], keep='first')\n",
    "    fin = fin_df.reset_index(drop = True)\n",
    "    \n",
    "    postdata = fin.to_dict(orient=\"index\")\n",
    "    config = {\n",
    "        \"apiKey\": \"AIzaSyDIo8bt7OrCX6KYaxplvUauQdaehcjUo_0\",\n",
    "        \"authDomain\": \"activity-crawling.firebaseapp.com\",\n",
    "        \"databaseURL\": \"https://activity-crawling-default-rtdb.firebaseio.com\",\n",
    "        \"projectId\": \"activity-crawling\",\n",
    "        \"storageBucket\": \"activity-crawling.appspot.com\",\n",
    "        \"messagingSenderId\": \"608978503357\",\n",
    "        \"appId\": \"1:608978503357:web:374a269b8fa1a64888d9d4\"}\n",
    "\n",
    "    firebase = pyrebase.initialize_app(config)\n",
    "    db = firebase.database()\n",
    "    db.child().update(postdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regular-gospel",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crawling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c5c7fe4303a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtofb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-4290f9f93212>\u001b[0m in \u001b[0;36mtofb\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtofb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mfinal_dreams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawling_ck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crawling' is not defined"
     ]
    }
   ],
   "source": [
    "tofb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-underwear",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-private",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "confident-grant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>qualification</th>\n",
       "      <th>title</th>\n",
       "      <th>notice</th>\n",
       "      <th>deadline</th>\n",
       "      <th>dday</th>\n",
       "      <th>sponsor</th>\n",
       "      <th>title2</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>공모전</td>\n",
       "      <td>대학(원)생</td>\n",
       "      <td>제9회 엔지니어링산업설계대전</td>\n",
       "      <td>21-02-01</td>\n",
       "      <td>21-03-19</td>\n",
       "      <td>30</td>\n",
       "      <td>산업통상자원부</td>\n",
       "      <td>제9회 엔지니어링산업설계대전</td>\n",
       "      <td>https://thinkyou.co.kr/contest/sector_view.asp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>공모전</td>\n",
       "      <td>대학(원)생</td>\n",
       "      <td>2021 ICT 융합 프로젝트 공모전</td>\n",
       "      <td>21-02-01</td>\n",
       "      <td>21-03-31</td>\n",
       "      <td>42</td>\n",
       "      <td>(주)엔티렉스-디바이스마트</td>\n",
       "      <td>2021 ICT 융합 프로젝트 공모전</td>\n",
       "      <td>https://thinkyou.co.kr/contest/sector_view.asp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>장학금</td>\n",
       "      <td>대학생</td>\n",
       "      <td>국가장학금 (2021년 1학기)</td>\n",
       "      <td>2021. 02. 03</td>\n",
       "      <td>2021. 03. 16</td>\n",
       "      <td>27</td>\n",
       "      <td>한국장학재단</td>\n",
       "      <td>국가장학금 (2021년 1학기)</td>\n",
       "      <td>https://www.dreamspon.com/scholarship/scholars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>장학금</td>\n",
       "      <td>대학생</td>\n",
       "      <td>국가근로장학 (2021년 1학기)</td>\n",
       "      <td>2021. 02. 03</td>\n",
       "      <td>2021. 03. 16</td>\n",
       "      <td>27</td>\n",
       "      <td>한국장학재단</td>\n",
       "      <td>국가근로장학 (2021년 1학기)</td>\n",
       "      <td>https://www.dreamspon.com/scholarship/scholars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>장학금</td>\n",
       "      <td>대학생</td>\n",
       "      <td>서울군위학사 입사생 선발</td>\n",
       "      <td>2021. 01. 11.</td>\n",
       "      <td>2021. 02. 17</td>\n",
       "      <td>0</td>\n",
       "      <td>군위군교육발전위원회</td>\n",
       "      <td>서울군위학사 입사생 선발</td>\n",
       "      <td>https://www.dreamspon.com/scholarship/scholars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>공모전</td>\n",
       "      <td>일반인, 국내외 석학과 연구진</td>\n",
       "      <td>제3회 글로벌 이노베이션 콘테스트(GIC)</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>42</td>\n",
       "      <td>LG화학</td>\n",
       "      <td>제3회 글로벌 이노베이션 콘테스트(GIC)</td>\n",
       "      <td>https://www.thinkcontest.com//Contest/ContestD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>공모전</td>\n",
       "      <td>대학생, 대학원생, 일반인</td>\n",
       "      <td>인공지능 Learner 및 인턴(Storytelling 학습자/개발자/기획자) 후보...</td>\n",
       "      <td>2021-01-09</td>\n",
       "      <td>2021-04-03</td>\n",
       "      <td>45</td>\n",
       "      <td>ISE, AIQ</td>\n",
       "      <td>인공지능 Learner 및 인턴(Storytelling 학습자/개발자/기획자) 후보...</td>\n",
       "      <td>https://www.thinkcontest.com//Contest/ContestD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>공모전</td>\n",
       "      <td>대학생, 대학원생</td>\n",
       "      <td>2020 ZEISS Microscopy Research Contest</td>\n",
       "      <td>2020-11-04</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>11</td>\n",
       "      <td>ZEISS</td>\n",
       "      <td>2020 ZEISS Microscopy Research Contest</td>\n",
       "      <td>https://www.thinkcontest.com//Contest/ContestD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>경진대회</td>\n",
       "      <td>대학(원)생</td>\n",
       "      <td>스타트업과 함께하는 피버팅 해커톤</td>\n",
       "      <td>21. 02. 26</td>\n",
       "      <td>21. 03. 08</td>\n",
       "      <td>0</td>\n",
       "      <td>한국무역협회</td>\n",
       "      <td>스타트업과 함께하는 피버팅 해커톤</td>\n",
       "      <td>https://www.contestkorea.com/sub/view.php?disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>경진대회</td>\n",
       "      <td>대학(원)생</td>\n",
       "      <td>2021 다솜이 드림메이커스</td>\n",
       "      <td>21. 01. 13</td>\n",
       "      <td>21. 12. 31</td>\n",
       "      <td>0</td>\n",
       "      <td>생명보험사회공헌재단, 교보생명</td>\n",
       "      <td>2021 다솜이 드림메이커스</td>\n",
       "      <td>https://www.contestkorea.com/sub/view.php?disp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    type     qualification                                              title  \\\n",
       "0    공모전            대학(원)생                                    제9회 엔지니어링산업설계대전   \n",
       "1    공모전            대학(원)생                               2021 ICT 융합 프로젝트 공모전   \n",
       "2    장학금               대학생                                  국가장학금 (2021년 1학기)   \n",
       "3    장학금               대학생                                 국가근로장학 (2021년 1학기)   \n",
       "4    장학금               대학생                                      서울군위학사 입사생 선발   \n",
       "..   ...               ...                                                ...   \n",
       "63   공모전  일반인, 국내외 석학과 연구진                            제3회 글로벌 이노베이션 콘테스트(GIC)   \n",
       "64   공모전    대학생, 대학원생, 일반인  인공지능 Learner 및 인턴(Storytelling 학습자/개발자/기획자) 후보...   \n",
       "65   공모전         대학생, 대학원생             2020 ZEISS Microscopy Research Contest   \n",
       "66  경진대회            대학(원)생                                 스타트업과 함께하는 피버팅 해커톤   \n",
       "67  경진대회            대학(원)생                                    2021 다솜이 드림메이커스   \n",
       "\n",
       "            notice       deadline dday           sponsor  \\\n",
       "0         21-02-01       21-03-19   30           산업통상자원부   \n",
       "1         21-02-01       21-03-31   42    (주)엔티렉스-디바이스마트   \n",
       "2     2021. 02. 03   2021. 03. 16   27            한국장학재단   \n",
       "3     2021. 02. 03   2021. 03. 16   27            한국장학재단   \n",
       "4   2021. 01. 11.    2021. 02. 17    0        군위군교육발전위원회   \n",
       "..             ...            ...  ...               ...   \n",
       "63      2021-01-25     2021-03-31   42              LG화학   \n",
       "64      2021-01-09     2021-04-03   45          ISE, AIQ   \n",
       "65      2020-11-04     2021-02-28   11             ZEISS   \n",
       "66      21. 02. 26     21. 03. 08    0            한국무역협회   \n",
       "67      21. 01. 13     21. 12. 31    0  생명보험사회공헌재단, 교보생명   \n",
       "\n",
       "                                               title2  \\\n",
       "0                                    제9회 엔지니어링산업설계대전    \n",
       "1                               2021 ICT 융합 프로젝트 공모전    \n",
       "2                                   국가장학금 (2021년 1학기)   \n",
       "3                                  국가근로장학 (2021년 1학기)   \n",
       "4                                       서울군위학사 입사생 선발   \n",
       "..                                                ...   \n",
       "63                            제3회 글로벌 이노베이션 콘테스트(GIC)   \n",
       "64  인공지능 Learner 및 인턴(Storytelling 학습자/개발자/기획자) 후보...   \n",
       "65             2020 ZEISS Microscopy Research Contest   \n",
       "66                                 스타트업과 함께하는 피버팅 해커톤   \n",
       "67                                    2021 다솜이 드림메이커스   \n",
       "\n",
       "                                                 link  \n",
       "0   https://thinkyou.co.kr/contest/sector_view.asp...  \n",
       "1   https://thinkyou.co.kr/contest/sector_view.asp...  \n",
       "2   https://www.dreamspon.com/scholarship/scholars...  \n",
       "3   https://www.dreamspon.com/scholarship/scholars...  \n",
       "4   https://www.dreamspon.com/scholarship/scholars...  \n",
       "..                                                ...  \n",
       "63  https://www.thinkcontest.com//Contest/ContestD...  \n",
       "64  https://www.thinkcontest.com//Contest/ContestD...  \n",
       "65  https://www.thinkcontest.com//Contest/ContestD...  \n",
       "66  https://www.contestkorea.com/sub/view.php?disp...  \n",
       "67  https://www.contestkorea.com/sub/view.php?disp...  \n",
       "\n",
       "[68 rows x 9 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-convertible",
   "metadata": {},
   "source": [
    "# 여기서부터 영준씨 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "british-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def incruit():\n",
    "#     url='https://gongmo.incruit.com/list/gongmolist.asp?ct=1&category=11'\n",
    "#     req=urllib.request.urlopen(url)\n",
    "#     res=req.read()\n",
    "#     soup=BeautifulSoup(res,'html.parser')\n",
    "#     data_list = soup.find(id='tbdyGmScrap').find_all('a')\n",
    "#     for data in data_list:\n",
    "#         req = requests.get(data.get('href'))\n",
    "#         soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "#         tmp = soup.find(class_='tBrd1Gray').find_all('td')\n",
    "#         title = soup.find(class_='job_new_top_title').get_text()\n",
    "#         term = tmp[3].get_text()\n",
    "#         classify = tmp[0].get_text().replace(\"<br/>\", \",\")\n",
    "#         host = tmp[1].get_text()\n",
    "#         link = tmp[4].find('a').get('href').replace('\\t', '')\n",
    "#         contest_in= [title, term, classify, host, link]\n",
    "\n",
    "#         start_bef, end_bef=term.split('~')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collectible-sleep",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'contest_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-84dec9672895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontest_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'contest_in' is not defined"
     ]
    }
   ],
   "source": [
    "# data=pd.DataFrame(contests)\n",
    "# data1=data.transpose()\n",
    "# del data1[2]\n",
    "# data1.loc[:,'notice'] = [start_bef]\n",
    "# data1.loc[:,'deadline']=[end_bef]\n",
    "# del data1[1]\n",
    "# data2['type']='공모전'\n",
    "# data2['Qualification']='대학(원)생'\n",
    "# data2['title2']=data2['title']\n",
    "# today=datetime.date.today()\n",
    "# targetday=datetime.date(2021,10,4)\n",
    "# #delta=targetday-today\n",
    "# #data2['dday']=delta.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-migration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-tsunami",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-maldives",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "classified-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tofb():\n",
    "    df1 = crawling()\n",
    "    df2 =  final_dreams()\n",
    "    df3 = browser2()\n",
    "    df4 = data2\n",
    "    \n",
    "    mid = pd.concat([df1, df2,df3,df4]) \n",
    "    mid_df = mid.reset_index(drop=True)\n",
    "    mid_df['title'] = mid_df['title'].str.strip()\n",
    "    fin_df = mid_df.drop_duplicates(['title'], keep='first')\n",
    "    fin = fin_df.reset_index(drop = True)\n",
    "    \n",
    "\n",
    "    postdata = fin.to_dict(orient=\"index\")\n",
    "    config = {\n",
    "        \"apiKey\": \"AIzaSyDIo8bt7OrCX6KYaxplvUauQdaehcjUo_0\",\n",
    "        \"authDomain\": \"activity-crawling.firebaseapp.com\",\n",
    "        \"databaseURL\": \"https://activity-crawling-default-rtdb.firebaseio.com\",\n",
    "        \"projectId\": \"activity-crawling\",\n",
    "        \"storageBucket\": \"activity-crawling.appspot.com\",\n",
    "        \"messagingSenderId\": \"608978503357\",\n",
    "        \"appId\": \"1:608978503357:web:374a269b8fa1a64888d9d4\"}\n",
    "\n",
    "    firebase = pyrebase.initialize_app(config)\n",
    "    db = firebase.database()\n",
    "    db.child().update(postdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "frozen-athens",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crawling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c5c7fe4303a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtofb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-24a8faf5b202>\u001b[0m in \u001b[0;36mtofb\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtofb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mfinal_dreams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crawling' is not defined"
     ]
    }
   ],
   "source": [
    "tofb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-registrar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
